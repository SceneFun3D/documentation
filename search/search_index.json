{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>      SceneFun3D  Fine-Grained Functionality and Affordance Understanding  in 3D Scenes <p> Alexandros Delitzas<sup>1</sup>,      Ay\u00e7a Takmaz<sup>1</sup> Federico Tombari<sup>2,3</sup>,      Robert W. Sumner<sup>1</sup>,      Marc Pollefeys<sup>1,4</sup>,     Francis Engelmann<sup>1,2</sup> <sup>1</sup>ETH Zurich,      <sup>2</sup>Google,      <sup>3</sup>TUM,     <sup>4</sup>Microsoft   </p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"#news","title":"News \ud83d\udce2","text":"<p>Check the Changelog for detailed updates (Last update: 10/03/2025)</p> <ul> <li>March 10, 2025: The benchmark submission portal is now live. Check out the updated submission instructions and the changelog.</li> <li>October 10, 2024: Initial release.</li> </ul>"},{"location":"#quick-links","title":"Quick links \ud83d\udd17","text":"<ul> <li>Data Downloader</li> <li>Project page</li> <li>Paper</li> <li>Toolkit</li> <li>Benchmark submission page</li> <li>Benchmark instructions</li> </ul>"},{"location":"#bibtex-citation","title":"BibTeX Citation \ud83d\ude4f","text":"<p>If you find our work useful for your research, please consider citing as:</p> <pre><code>@inproceedings{delitzas2024scenefun3d, \n  title = {{SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes}}, \n  author = {Delitzas, Alexandros and Takmaz, Ayca and Tombari, Federico and Sumner, Robert and Pollefeys, Marc and Engelmann, Francis}, \n  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, \n  year = {2024}\n}\n</code></pre>"},{"location":"annotation-tool/","title":"SceneFun3D annotation tool","text":"<p>Coming soon.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#0.2.0","title":"0.2.0 March 10, 2024","text":"<ul> <li>Benchmark submission portal on EvalAI</li> <li>Updated benchmark instructions</li> <li>Changed submission format to Run-Length Encoding (RLE) for improved efficiency. Thanks to ScanNet++ for the inspiration and to @elisabettafedele for the implementation</li> <li>Nerfstudio format helpers</li> <li>Fixed issues #5 (thanks to @DennisRotondi) and #8, #12 (thanks to @sankleta). This involved changes to the following assets: <code>468140/47334514/hires_wide</code> (added), <code>464758/44358731/hires_depth</code> (added), <code>420693/420693_annotations.json</code> (modified), <code>420673/420673_descriptions.json</code> (modified) and <code>467283/467283_annotations.json</code> (modified)</li> </ul>"},{"location":"changelog/#0.1.0","title":"0.1.0 October 10, 2024","text":"<ul> <li>Initial release</li> <li>Dataset includes 315 scenes (200 training scenes, 30 validation scenes, 85 test scenes)</li> <li>Toolkit code</li> <li>Documentation page</li> </ul>"},{"location":"contact-us/","title":"Contact Us","text":"<p>For any technical issues, questions or feature requests, please raise an issue on the Github repo.</p> <p>For direct contact, or any concerns: email us.</p>"},{"location":"contributing/","title":"Contributing to SceneFun3D","text":"<p>Thank you for considering contributing to SceneFun3D! We welcome all contributions, whether it's bug fixes, feature additions, documentation improvements, feature suggestions, or any other enhancement.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#1-fork-the-repository","title":"1. Fork the Repository","text":"<p>Start by forking the repository you want to contribute to and cloning it to your local machine:</p> SceneFun3D Toolkit<pre><code>git clone https://github.com/SceneFun3D/scenefun3d.git\ncd scenefun3d\n</code></pre> SceneFun3D Documentation<pre><code>git clone https://github.com/SceneFun3D/documentation.git\ncd documentation\n</code></pre>"},{"location":"contributing/#2-create-a-new-branch","title":"2. Create a New Branch","text":"<p>Always create a new branch to work on, based on the <code>main</code> branch:</p> <pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#3-make-changes","title":"3. Make Changes","text":"<p>Make your desired changes. Ensure your code follows the project's style and conventions.</p>"},{"location":"contributing/#4-write-tests","title":"4. Write Tests","text":"<p>If applicable, write tests for your changes to ensure everything works as expected. Ensure all tests pass before submitting.</p>"},{"location":"contributing/#5-commit-your-changes","title":"5. Commit Your Changes","text":"<p>Make meaningful, well-documented commit messages. Use your preferred name for Git commits.</p> <pre><code>git commit -m \"Add feature description\"\n</code></pre>"},{"location":"contributing/#6-push-to-your-fork","title":"6. Push to Your Fork","text":"<p>Push your branch to your forked repository:</p> <pre><code>git push origin feature/your-feature-name\n</code></pre>"},{"location":"contributing/#7-open-a-pull-request","title":"7. Open a Pull Request","text":"<p>Go to the original repository and open a pull request. Provide a clear and concise description of your changes, referencing any related issues or discussions.</p>"},{"location":"contributing/#feedback-and-support","title":"Feedback and Support","text":"<p>We appreciate all feedback! Feel free to open an issue if you have questions or need assistance.</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>Welcome to the official SceneFun3D documentation! </p> <p>This documentation provides a comprehensive guide to help you efficiently integrate the SceneFun3D dataset into your research. It includes detailed descriptions of the data assets, step-by-step instructions for downloading the data, utilizing the project\u2019s tools and libraries, and guidelines for running the evaluation benchmarks.</p>"},{"location":"getting-started/#contents","title":"Contents","text":"<p>Documentation is organized into 4 parts:</p> <ul> <li> <p>Dataset: This section covers everything related to the SceneFun3D dataset. The documentation explains the data assets, format and organization.</p> </li> <li> <p>Toolkit: This section provides essential tools to work with the SceneFun3D dataset efficiently. It includes utilities for downloading, parsing and visualizing the data. Additionally, you'll find example codes to help you get started quickly and easily integrate these tools into your workflow.</p> </li> <li> <p>Annotation tool: This section provides step-by-step instructions for setting up the annotation tool, navigating its features and annotation interfaces.</p> </li> <li> <p>Benchmarks: In the benchmarks section, we describe the proposed tasks and provide general evaluation guidelines, including instructions for local evaluation and how to submit results to our online benchmarks.</p> </li> </ul>"},{"location":"license/","title":"License","text":""},{"location":"license/#data","title":"Data","text":"<p>The SceneFun3D data assets are under the CC-BY-4.0 license.</p> <p>Important Note: As the SceneFun3D dataset is built upon the ARKitScenes dataset, by following the instructions to download the data, you also agree with the license &amp; terms and conditions of the ARKitScenes dataset, as well as the code of conduct provided in the ARKitScenes repository. </p>"},{"location":"license/#toolkit","title":"Toolkit","text":"<p>The SceneFun3D toolkit code is under the MIT license.</p>"},{"location":"benchmarks/guidelines/","title":"Evaluation guidelines","text":"<p>The SceneFun3D training and validation sets can be utilized to train and evaluate models locally. For evaluation on the validation set, we provide evaluation scripts in the SceneFun3D toolkit for each task here. </p> <p>Currently, the benchmark is evaluated using version 0.2.0 of the dataset.</p> <p>Benchmark results are evaluated on the hidden test set for which we do not provide the ground-truth annotations. The benchmark is hosted on EvalAI and can be found here. </p> <p>Prior to making a submission on the evaluation benchmark, make sure the submission is in the correct format by following the instructions outlined for each task. Otherwise, the submission will fail.</p> <p>On the benchmark submission page, you can submit to both the validation and test splits. Since evaluation on the validation split can also be performed locally, this step acts as an optional sanity check. Only test split submissions count towards official benchmarking.</p> <p>In the sections below, you can find information about evaluation and benchmark submissions and description for each task:</p> <ul> <li>Task 1: Functionality segmentation</li> <li>Task 2: Task-driven affordance grounding</li> <li>Task 3: Motion estimation</li> </ul>"},{"location":"benchmarks/task1/","title":"Task 1: Functionality segmentation","text":""},{"location":"benchmarks/task1/#task-description","title":"Task description","text":"<p>In the first track of the SceneFun3D benchmark, we propose the following challenge:</p> <p>Functionality segmentation</p> <p>TASK: Segment the functional interactive elements in a 3D scene.</p> <p>INPUT: The Faro laser scan of a given scene, multiple RGB-D sequences captured with an iPad Pro and camera parameters.</p> <p>OUTPUT: Instance segmentation of the point cloud that corresponds to the vertices of the provided laser scan, identifying the functional interactive elements along with predictions of affordance labels.</p>"},{"location":"benchmarks/task1/#submission-instructions","title":"Submission Instructions","text":"<p>Given a 3D scene, the participants are tasked with segmenting the instances of functional interactive elements in the scene. Expected result is functional interactive element masks, predicted affordance classes and confidence scores for each mask. </p> <p>We ask the participants to upload their results as a single <code>.zip</code> file, which when unzipped must contain the prediction files in the root. There must not be any additional files or folders in the archive except those specified below.</p> <p>Results must be provided as a text file for each scene. Each text file should contain a line for each instance, containing the relative path to the instance mask file that specifies the mask indices, the predicted affordance class ID and the confidence of the prediction. The result text files must be named according to the corresponding laser scan (<code>visit_id</code>) as <code>{visit_id}.txt</code>. Predicted <code>.txt</code> files listing the instances of each scan must live in the root of the unzipped submission. Predicted instance mask files must live in a subdirectory named <code>predicted_masks/</code> of the unzipped submission. For example, a submission should look like the following:</p> <pre><code>/PATH/TO/RESULTS/\n    |__ {visit_id_1}.txt\n    |__ {visit_id_2}.txt \n         \u22ee\n    |__ {visit_id_N}.txt\n    |__ predicted_masks/\n        |__ {visit_id_1}_000.txt\n        |__ {visit_id_1}_001.txt\n            \u22ee\n</code></pre> <p>for all the available laser scans.</p> <p>Each prediction file for a scene should contain a list of instances, where an instance is: (1) the relative path to the predicted mask file, (2) the affordance class ID and (3) the float confidence score. If your method does not produce confidence scores, you can use <code>1.0</code> as the confidence score for all masks. Each line in the prediction file should correspond to one instance, and the two values above separated by a space. Thus, the filenames in the prediction files must not contain spaces. The predicted instance mask file should provide the mask vertex indices of the provided laser scan, i.e., <code>{visit_id}_laser_scan.ply</code>, following the original order of the vertices in this file. Following ScanNet++, we use Run-Length Encoding (RLE) to efficiently encode instance masks. Each instance mask file contains the RLE representation of the predicted mask over the vertices of the laser scan <code>{visit_id}_laser_scan.ply</code>, stored as pairs of start and length values. start is the 1-indexed position of a contiguous group of vertices in the point cloud, and length is the number of vertices in that group. For example, the RLE encoding of the 12-length binary mask <code>[0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0]</code> is: <code>'2 2 6 3 10 2'</code>.</p> <p>Consider a scene identified by visit_id <code>123456</code>. In this case, the submission files could look like:</p> <p><code>123456.txt</code> <pre><code>predicted_masks/123456_000.txt 5 0.7234\npredicted_masks/123456_001.txt 3 0.9038\n\u22ee\n</code></pre></p> <p>and <code>predicted_masks/123456_000.txt</code> could look like: <pre><code>2123 4 2235 3 3724 12 ...\n</code></pre></p> <p>where <code>predicted_masks/123456_000.txt</code> contains a single line with the RLE-encoded instance mask, where each pair corresponds to a start index and run length.</p> <p>As a utility, we provide a script that generates an example submission by reading the ground-truth data of the validation set in the evaluation format. This example submission achieves a perfect score on the benchmark's validation set.</p> <p>Note</p> <p>The prediction files must adhere to the vertex ordering of the original laser scan point cloud <code>{visit_id}_laser_scan.ply</code>.  If your pipeline alters this vertex ordering (e.g., through cropping the laser scan using the <code>crop_mask</code> data asset),  ensure that the model predictions are re-ordered to match the original vertex ordering before generating the prediction files.</p> <p>Note</p> <p>Functional interactive elements which are made of transparent or reflective material resulting in bad 3D geometry  are excluded from the evaluation procedure. The model's predictions on these areas are excluded as well.</p> <p>The table below lists the ID for each affordance class:</p> <p> Label Class ID rotate 1 key_press 2 tip_push 3 hook_pull 4 pinch_pull 5 hook_turn 6 foot_push 7 plug_in 8 unplug 9 <p></p>"},{"location":"benchmarks/task1/#local-evaluation","title":"Local evaluation","text":"<p>For local evaluation on the validation set, GT annotations need to be extracted in a specific format. To do this extraction, you can run:</p> <pre><code>python -m eval.functionality_segmentation.prepare_gt_val_data --data_dir &lt;data_dir&gt; --val_scenes_list &lt;val_scenes_list_path&gt; --out_gt_dir &lt;out_gt_dir&gt;\n</code></pre> <p>where:</p> <ul> <li><code>--data_dir &lt;data_dir&gt;</code>: Specify the directory where data is stored</li> <li><code>--val_scenes_list &lt;val_scenes_list_path&gt;</code>: Specify the list of scenes that will be used for validation as a line-separated .txt file</li> <li><code>--out_gt_dir &lt;out_gt_dir&gt;</code>: Specify the directory where the processed GT annotations will be stored </li> </ul> <p>For evaluation, run:</p> <p><pre><code>python -m eval.functionality_segmentation.evaluate --pred_dir &lt;pred_dir&gt; --gt_dir &lt;gt_dir&gt;\n</code></pre> where:</p> <ul> <li><code>--pred_dir &lt;pred_dir&gt;</code>: Specify the directory where the model predictions are stored in the correct submission format.</li> <li><code>--gt_dir &lt;gt_dir&gt;</code>: Specify the directory where the processed GT annotations are stored. </li> </ul>"},{"location":"benchmarks/task1/#online-benchmark","title":"Online benchmark","text":"<p>You can upload the <code>.zip</code> file containing your model's prediction on our online benchmark which performs evaluation on the hidden test set.</p> <p>Please make sure that it follows the correct submission format. Otherwise, the submission will fail.</p> <p>To create the <code>.zip</code> file for submission, you can run: <pre><code>cd path/to/submission/files\nzip -r submission.zip *\n</code></pre></p>"},{"location":"benchmarks/task1/#evaluation-metrics","title":"Evaluation metrics","text":"<p>Submissions are evaluated using the mean Average Precision at the IoU thresholds of 0.25 and 0.50, denoted as AP<sub>25</sub> and AP<sub>50</sub> respectively. Additionally, they are assessed using the average across IoU thresholds from 0.5 to 0.95 with a step of 0.05 (AP). Submissions are ranked based on the AP metric.</p>"},{"location":"benchmarks/task2/","title":"Task 2: Task-driven affordance grounding","text":""},{"location":"benchmarks/task2/#task-description","title":"Task description","text":"<p>In the second track of the SceneFun3D benchmark, we propose the following challenge:</p> <p>Task-driven affordance grounding</p> <p>TASK: Given a text-based description of a task, the aim is to localize and segment the functional interactive elements that an agent needs to interact with to successfully accomplish the task. </p> <p>INPUT: The Faro laser scan of a given scene, multiple RGB-D sequences captured with an iPad Pro, camera parameters, and a language description of a task.</p> <p>OUTPUT: Instance segmentation of the point cloud that corresponds to the vertices of the provided laser scan, segmenting the functional interactive elements that the agent needs to manipulate.</p> <p>We highlight that, in some cases, more than one instance of functional interactive elements may correspond to a single language task description.</p>"},{"location":"benchmarks/task2/#benchmark-test-set","title":"Benchmark test set","text":"<p>For the benchmark test set, we provide input language task descriptions without any ground-truth annotations. Participants must run their models on these inputs and submit their predictions via our submission page, following the provided submission instructions. You can find the input language task descriptions here. The format is as follows:</p> <pre><code>[\n  {\n    \"visit_id\": the identifier of the scene,\n    \"desc_id\": unique id of the description,\n    \"description\": language instruction of the task\n  }, \n  ...\n]\n</code></pre>"},{"location":"benchmarks/task2/#submission-instructions","title":"Submission Instructions","text":"<p>Given a language task description, the participants are asked to segment functional interactive element instances that an agent needs to interact with to successfully accomplish the task. Expected result is functional interactive element masks, and confidence scores for each mask. </p> <p>We ask the participants to upload their results as a single <code>.zip</code> file, which when unzipped must contain the prediction files in the root. There must not be any additional files or folders in the archive except those specified below.</p> <p>Results must be provided as a text file for each scene. Each text file should contain a line for each instance, containing the relative path to the instance mask file that specifies the mask indices, and the confidence of the prediction. The result text files must be named according to the corresponding laser scan (<code>visit_id</code>) and language description (<code>desc_id</code>), as <code>{visit_id}_{desc_id}.txt</code>. Predicted <code>.txt</code> files listing the instances of each scan must live in the root of the unzipped submission. Predicted instance mask files must live in a subdirectory named <code>predicted_masks/</code> of the unzipped submission. For example, a submission should look like the following:</p> <pre><code>/PATH/TO/RESULTS/\n    |__ {visit_id_1}_{desc_id_1}.txt\n    |__ {visit_id_2}_{desc_id_2}.txt \n         \u22ee\n    |__ {visit_id_N}_{desc_id_N}.txt\n    |__ predicted_masks/\n        |__ {visit_id_1}_{desc_id_1}_000.txt\n        |__ {visit_id_1}_{desc_id_1}_001.txt\n            \u22ee\n</code></pre> <p>for all the available N pairs (laser scan, language description).</p> <p>Each prediction file for a scene should contain a list of instances, where an instance is: (1) the relative path to the predicted mask file, (2) the float confidence score. If your method does not produce confidence scores, you can use <code>1.0</code> as the confidence score for all masks. Each line in the prediction file should correspond to one instance, and the two values above separated by a space. Thus, the filenames in the prediction files must not contain spaces. The predicted instance mask file should provide the mask vertex indices of the provided laser scan, i.e., <code>{visit_id}_laser_scan.ply</code>, following the original order of the vertices in this file. Following ScanNet++, we use Run-Length Encoding (RLE) to efficiently encode instance masks. Each instance mask file contains the RLE representation of the predicted mask over the vertices of the laser scan <code>{visit_id}_laser_scan.ply</code>, stored as pairs of start and length values. start is the 1-indexed position of a contiguous group of vertices in the point cloud, and length is the number of vertices in that group. For example, the RLE encoding of the 12-length binary mask <code>[0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0]</code> is: <code>'2 2 6 3 10 2'</code>.</p> <p>Consider a scene identified by visit_id <code>123456</code>, with a language description input identified by desc_id <code>5baea371-b33b-4076-92b1-587a709e6c65</code>. In this case, the submission files could look like:</p> <p><code>123456_5baea371-b33b-4076-92b1-587a709e6c65.txt</code> <pre><code>predicted_masks/123456_5baea371-b33b-4076-92b1-587a709e6c65_000.txt 0.7234\npredicted_masks/123456_5baea371-b33b-4076-92b1-587a709e6c65_001.txt 0.9038\n\u22ee\n</code></pre></p> <p>and <code>predicted_masks/123456_5baea371-b33b-4076-92b1-587a709e6c65_000.txt</code> could look like: <pre><code>2123 4 2235 3 3724 12 ...\n</code></pre></p> <p>where <code>predicted_masks/123456_000.txt</code> contains a single line with the RLE-encoded instance mask, where each pair corresponds to a start index and run length.</p> <p>As a utility, we provide a script that generates an example submission by reading the ground-truth data of the validation set in the evaluation format. This example submission achieves a perfect score on the benchmark's validation set.</p> <p>Note</p> <p>The prediction files must adhere to the vertex ordering of the original laser scan point cloud <code>{visit_id}_laser_scan.ply</code>.  If your pipeline alters this vertex ordering (e.g., through cropping the laser scan using the <code>crop_mask</code> data asset),  ensure that the model predictions are re-ordered to match the original vertex ordering before generating the prediction files.</p> <p>Note</p> <p>Functional interactive elements which are made of transparent or reflective material resulting in bad 3D geometry  are excluded from the evaluation procedure. The model's predictions on these areas are excluded as well.</p>"},{"location":"benchmarks/task2/#local-evaluation","title":"Local evaluation","text":"<p>For local evaluation on the validation set, GT annotations need to be extracted in a specific format. To do this extraction, you can run:</p> <pre><code>python -m eval.affordance_grounding.prepare_gt_val_data --data_dir &lt;data_dir&gt; --val_scenes_list &lt;val_scenes_list_path&gt; --out_gt_dir &lt;out_gt_dir&gt;\n</code></pre> <p>where:</p> <ul> <li><code>--data_dir &lt;data_dir&gt;</code>: Specify the directory where data is stored.</li> <li><code>--val_scenes_list &lt;val_scenes_list_path&gt;</code>: Specify the list of scenes that will be used for validation as a line-separated .txt file.</li> <li><code>--out_gt_dir &lt;out_gt_dir&gt;</code>: Specify the directory where the processed GT annotations will be stored.</li> </ul> <p>For evaluation, run:</p> <p><pre><code>python -m eval.affordance_grounding.evaluate --pred_dir &lt;pred_dir&gt; --gt_dir &lt;gt_dir&gt;\n</code></pre> where:</p> <ul> <li><code>--pred_dir &lt;pred_dir&gt;</code>: Specify the directory (unzipped) where the model predictions are stored in the submission format.</li> <li><code>--gt_dir &lt;gt_dir&gt;</code>: Specify the directory where the processed GT annotations are stored. </li> </ul>"},{"location":"benchmarks/task2/#online-benchmark","title":"Online benchmark","text":"<p>You can upload the <code>.zip</code> file containing your model's prediction on our online benchmark which performs evaluation on the hidden test set.</p> <p>Please make sure that it follows the correct submission format. Otherwise, the submission will fail.</p> <p>To create the <code>.zip</code> file for submission, you can run: <pre><code>cd path/to/submission/files\nzip -r submission.zip *\n</code></pre></p>"},{"location":"benchmarks/task2/#evaluation-metrics","title":"Evaluation metrics","text":"<p>Submissions are evaluated using the mean Average Precision at the IoU thresholds of 0.25 and 0.50, denoted as AP<sub>25</sub> and AP<sub>50</sub> respectively. Submissions are ranked based on the AP<sub>50</sub> metric.</p>"},{"location":"benchmarks/task3/","title":"Task 3: Motion estimation","text":"<p>Coming soon.</p>"},{"location":"dataset/annotations/","title":"Annotation data","text":"<p>Currently, SceneFun3D provides three categories of annotated data:</p> <ul> <li>Functional interactive element annotations</li> <li>Language task descriptions </li> <li>Motion annotations</li> </ul> <p>In the sections below, we describe the provided data for each category. Each annotation is  accompanied with a unique identifier of the form <code>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code>.</p>"},{"location":"dataset/annotations/#functional-interactive-elements","title":"Functional interactive elements","text":"<p>The format of the functional interactive element annotations can be seen below.</p> annotations.json<pre><code>{\n  \"visit_id\": the identifier of the scene,\n  \"annotations\": [\n    {\n      \"annot_id\": unique id of the annotation,\n      \"indices\": the mask indices of the original laser scan point cloud ({visit_id}_laser_scan.ply) that comprise the functional interactive element instance,\n      \"label\": affordance label\n    },\n    ...\n  ]\n}\n</code></pre> <p>Currently, the SceneFun3D dataset contains interactions with the following affordance labels:</p> Label Description rotate functionalities that are adjusted by a rotary switch knob, e.g., thermostat key_press surfaces that consist of keys that can be pressed, e.g., remote control, keyboard tip_push functionalities that can be triggered by the tip of the finger, e.g., light switch hook_pull surfaces that can be pulled by hooking up fingers, e.g., fridge handle pinch_pull surfaces that can be pulled through a pinch movement, e.g., drawer knob hook_turn surfaces that can be turned by hooking up fingers, e.g., door handle foot_push surfaces that can be pushed by foot, e.g., foot pedal of a trash can plug_in surfaces that comprise electrical power sources unplug removing a plug from a socket <p>In addition to these affordance categories, we have annotated functionalities whose geometry or the parent object\u2019s geometry is not well-captured in the laser scans (e.g., reflective or transparent surfaces) under the label <code>exclude</code>. These cases are excluded during the evaluation process.</p>"},{"location":"dataset/annotations/#language-task-descriptions","title":"Language task descriptions","text":"<p>The format of the natural language task descriptions can be seen below.</p> descriptions.json<pre><code>{\n  \"visit_id\": the identifier of the scene,\n  \"descriptions\": [\n    {\n      \"desc_id\": unique id of the description,\n      \"annot_id\": [\n        list of the associated annotation id's in the *annotations.json* file\n      ],\n      \"description\": language instruction of the task\n    },\n    ...\n  ]\n}\n</code></pre> <p>We highlight that, in some cases, more than one instance of functional interactive elements may correspond to a single language task description.</p>"},{"location":"dataset/annotations/#motion-annotations","title":"Motion annotations","text":"<p>The format of the motion annotations can be seen below.</p> motions.json<pre><code>{\n  \"visit_id\": the identifier of the scene,\n  \"motions\": [\n    {\n      \"motion_id\": unique id of the description,\n      \"annot_id\": the associated annotation id in the *annotations.json* file,\n      \"motion_type\": motion type (rotational or translational),\n      \"motion_dir\": motion direction (three element array),\n      \"motion_origin_idx\": point index of the original laser scan point cloud ({visit_id}_laser_scan.ply) which comprises the motion axis origin ,\n      \"motion_viz_orient\": motion visualization orientation (optional)`\n    },\n    ...\n  ]\n}\n</code></pre>"},{"location":"dataset/file-structure/","title":"Data assets and file structure","text":"<p>We represent each scene with a visit_id (6-digit number) and each video sequence with a video_id (8-digit number). For each scene, we provide a high-resolution point cloud generated by combining multiple Faro laser scans of the scene. Additionally, each scene is accompanied by  video sequences (three on average) recorded with a 2020 iPad Pro.</p>"},{"location":"dataset/file-structure/#file-structure","title":"File structure","text":"<p>Data is organized per visit_id as follows:</p> <p>\ud83d\udce6 PATH/TO/DATA/DIR/</p> <ul> <li>\u2523 \ud83d\udcc2 &lt;visit_id&gt;/</li> <ul> <li>\u2523 \ud83d\udcc4 &lt;visit_id&gt;_laser_scan.ply: combined Faro laser scan with 5mm resolution</li> <li>\u2523 \ud83d\udcc4 &lt;visit_id&gt;_crop_mask.npy: binary mask to crop extraneous points from the combined Faro laser scan</li> <li>\u2523 \ud83d\udcc4 &lt;visit_id&gt;_annotations.json: functional interactive element annotations</li> <li>\u2523 \ud83d\udcc4 &lt;visit_id&gt;_descriptions.json: natural language task descriptions</li> <li>\u2523 \ud83d\udcc4 &lt;visit_id&gt;_motions.json: motion annotations</li> <li>\u2517 \ud83d\udcc2 &lt;video_id&gt;/</li> <ul> <li>\u2523 \ud83d\udcc2 lowres_wide/: low resolution RGB frames of the wide camera (256x192) - 60 FPS</li> <ul> <li>\u2523 \ud83d\udcc4 &lt;video_id&gt;_&lt;timestamp&gt;.png: images (.png format) are indexed by timestamps</li> <li>\u2517 \u2022\u2022\u2022</li> </ul> <li>\u2523 \ud83d\udcc2 lowres_wide_intrinsics/: camera intrinsics for the low resolution frames</li> <ul> <li>\u2523 \ud83d\udcc4 &lt;video_id&gt;_&lt;timestamp&gt;.pincam: intrinsics files (.pincam format) are indexed by timestamps</li> <li>\u2517 \u2022\u2022\u2022</li> </ul> <li>\u2523 \ud83d\udcc2 lowres_depth/: depth images acquired with the iPad Lidar sensor (256x192) - 60 FPS</li> <ul> <li>\u2523 \ud83d\udcc4 &lt;video_id&gt;_&lt;timestamp&gt;.png: images (.png format in millimeters) are indexed by timestamps</li> <li>\u2517 \u2022\u2022\u2022</li> </ul> <li>\u2523 \ud83d\udcc4 lowres_poses.traj: contains the ARKit camera pose trajectory in the ARKit coordinate system</li> <li>\u2523 \ud83d\udcc2 hires_wide/: high resolution RGB images of the wide camera (1920x1440 in landscape mode, 1440x1920 in portrait mode) - 10 FPS</li> <ul> <li>\u2523 \ud83d\udcc4 &lt;video_id&gt;_&lt;timestamp&gt;.jpg: images (.jpg format) are indexed by timestamps</li> <li>\u2517 \u2022\u2022\u2022</li> </ul> <li>\u2523 \ud83d\udcc2 hires_wide_intrinsics/: camera intrinsics for the high resolution images</li> <ul> <li>\u2523 \ud83d\udcc4 &lt;video_id&gt;_&lt;timestamp&gt;.pincam: intrinsics files (.pincam format) are indexed by timestamps</li> <li>\u2517 \u2022\u2022\u2022</li> </ul> <li>\u2523 \ud83d\udcc2 hires_depth/: the ground-truth depth image projected from the mesh generated by Faro\u2019s laser scanners (1920x1440 in landscape mode, 1440x1920 in portrait mode) - 10 FPS</li> <ul> <li>\u2523 \ud83d\udcc4 &lt;video_id&gt;_&lt;timestamp&gt;.png: images (.png format in millimeters) are indexed by timestamps</li> <li>\u2517 \u2022\u2022\u2022</li> </ul> <li>\u2523 \ud83d\udcc4 hires_poses.traj: contains the camera poses estimated with COLMAP in the Faro laser scan coordinate system</li> <li>\u2523 \ud83d\udcc4 &lt;video_id&gt;_transform.npy: 4x4 transformation matrix that registers the Faro laser scan to the ARKit coordinate system</li> <li>\u2523 \ud83d\udcc4 &lt;video_id&gt;_3dod_mesh.ply: ARKit 3D mesh reconstruction</li> <li>\u2517 \ud83d\udcc4 &lt;video_id&gt;.{mov,mp4}: Video captured with ARKit (available in both .mov and .mp4 formats)</li> </ul> </ul> </ul>"},{"location":"dataset/file-structure/#data-download","title":"Data download","text":"<p>To download the data, please refer to Toolkit &gt; Data Downloader.</p>"},{"location":"dataset/file-structure/#annotation-data","title":"Annotation data","text":"<p>For each scene, the annotation data are provided in <code>annotation.json</code>, <code>descriptions.json</code> and <code>motions.json</code>. For information about the data format,  please refer to Dataset &gt; Annotations.</p>"},{"location":"dataset/file-structure/#laser-scan-and-arkit-coordinate-system","title":"Laser scan and ARKit coordinate system","text":"<p>ARKit related data assets are expressed in the ARKit coordinate system, while Faro related data assets are expressed in the Faro laser scan coordinate system.  Specifically, the ARKit-estimated camera poses and the ARKit 3D mesh reconstruction are expressed in the ARKit coordinate system, whereas the COLMAP-estimated camera poses and the Faro laser scan is expressed in the Faro coordinate system. We note that the ARKit coordinate systems among different video sequences of the same scene are not necessarily the same.</p> <p>The transformation matrix <code>&lt;video_id&gt;_transform.npy</code> can be used to register the Faro laser scan to the ARKit coordinate system.</p>"},{"location":"dataset/file-structure/#combined-faro-laser-scan","title":"Combined Faro laser scan","text":"<p>ARKitScenes provides multiple laser scans per scene (four on average) by placing a Faro Focus S70 laser scanner in different positions in the scene. We use the provided laser scanner\u2019s poses for each scene and combine the laser scans under the same coordinate system to increase the scene coverage. Afterwards, we downsample the combined laser scan with a voxel size of 5mm, which is sufficient to preserve the details of the functional interactive elements of the scene (e.g., small buttons, knobs, handles, etc.), while enabling processing by machine learning models. All annotations are performed on the combined laser scan. The resulting data asset is <code>&lt;visit_id&gt;_laser_scan.ply</code>.</p> <p>As the laser scan might include extraneous points due to transparent surfaces (e.g., windows), we provide a binary mask to crop them (<code>&lt;visit_id&gt;_crop_mask.npy</code>).</p>"},{"location":"dataset/file-structure/#ipad-video-sequences","title":"iPad video sequences","text":"<p>Each scene is accompanied with iPad video sequences (three on average) and the related data assets. As an improvement upon original ARKitScenes, we provide a higher number of registered high resolution frames for each video sequence along with accurate COLMAP-estimated poses and Faro-rendered depth maps. </p> <p>Furthermore, the provided high-resolution video frames are rotated so that the sky direction is up. As a result, frames may be in landscape (1920x1440) or portrait (1440x1920) orientation.</p>"},{"location":"dataset/file-structure/#arkit-versus-colmap-poses","title":"ARKit versus COLMAP poses","text":"<p>ARKit camera poses (<code>lowres_poses.traj</code>) are derived from the iPad's on-device ARKit world tracking. The original ARKitScenes dataset provides high-resolution iPad frames which are not temporally synchronized with these ARKit poses, thus rigid body motion interpolation needs to be performed to estimate the camera pose, which can introduce errors. When backprojecting from the iPad frames to the laser scan, this interpolation and ARKit\u2019s inherent inaccuracies can lead to misalignment errors of approximately 1-2 cm in some cases. </p> <p>To address this, we employ SuperGlue and COLMAP to estimate accurate camera poses in the Faro laser scan coordinate system (<code>hires_poses.traj</code>). As input for this pipeline, we sample high-resolution frames from the video sequences at 10 FPS (<code>hires_wide</code>).</p> <p>The camera poses are stored in .traj files. These files contain a line for each pose</p> <p>.traj files<pre><code>timestamp angle_axis_x angle_axis_y angle_axis_z translation_x translation_y translation_z\n...\n</code></pre> Column 1 contains the timestamp, columns 2-4 contain the rotation in angle-axis representation (in radians) and columns 5-7 contain the translation (in meters). These files can be easily parsed by utilizing our Toolkit's data parser.</p> <p>The .traj files can be easily parsed with our toolkit.</p>"},{"location":"dataset/file-structure/#camera-intrinsics","title":"Camera intrinsics","text":"<p>For each image, the intrinsic matrix is stored in a <code>.pincam</code> file. This file contains a single space-delimited line of text with the following fields:</p> <p>&lt;video_id&gt;_&lt;timestamp&gt;.pincam<pre><code>width height focal_length_x focal_length_y principal_point_x principal_point_y\n</code></pre> The .pincam files can be easily parsed with our toolkit.</p>"},{"location":"dataset/overview/","title":"The SceneFun3D dataset","text":"<p>The SceneFun3D dataset aims to catalyze research towards understanding functionalities and affodances in 3D environments. To this end, it provides fine-grained functional interaction annotations in high-fidelity reconstructions of real-world indoor spaces.</p>"},{"location":"dataset/overview/#annotations","title":"Annotations","text":"<p>The dataset contains three categories of annotations aiming to encourage research on the following questions:</p> <ul> <li>Where are the functionalities located in 3D indoor environments and what actions they afford?</li> <li>What purpose do the functionalities serve in the scene context?</li> <li>How to interact with the functional elements?</li> </ul>"},{"location":"dataset/overview/#functional-interactive-elements","title":"Functional interactive elements","text":"<p>SceneFun3D contains annotations of functional interactive elements (e.g., knobs, handles, buttons) which comprise a 3D instance mask  on the high-resolution 3D geometry reconstruction followed by an affordance label that described the action that the element affords.</p>"},{"location":"dataset/overview/#natural-language-task-descriptions","title":"Natural language task descriptions","text":"<p>Beyond localizing the functionalities, it is crucial to understand the purpose that they serve in the scene context for task-oriented scene interactions. The dataset provides diverse, free-form language descriptions of tasks that involve interacting with the annotated functionalities, which require common sense and multi-hop reasoning to interpret and handle effectively.</p>"},{"location":"dataset/overview/#motions","title":"Motions","text":"<p>Additionally, the dataset provides motion annotations required to manipulate the interactive elements.</p>"},{"location":"dataset/overview/#3d-indoor-environments","title":"3D indoor environments","text":"<p>SceneFun3D builds upon the ARKitScenes dataset, which offers a large-scale set of indoor scenes captured with a professional stationary Faro laser scanner and a commodity iPad device. To enhance its usability for 3D scene understanding applications,  we perform the following improvements and provide the related assets:</p> <ul> <li>Faro-ARKit coordinate system transformation</li> <li>Accurate COLMAP-estimated camera poses synced with the iPad video frames and Faro-rendered depth maps</li> <li>Increased number of registered high-resolution iPad RGB frames in the Faro coordinate system</li> <li>Easy-to-use data parser</li> </ul> <p> </p>"},{"location":"toolkit/data-downloader/","title":"Data downloader","text":"<p>We provide a data downloader script that downloads and prepares the data. </p> <p>You can run as:</p> <pre><code>python -m data_downloader.data_asset_download --split &lt;split&gt; --download_dir &lt;download_dir&gt; --download_only_one_video_sequence --dataset_assets &lt;identifier list of data assets to download&gt;\n</code></pre> <p>where the supported arguments are:</p> <ul> <li> <p><code>--split &lt;split&gt;</code>: Specify the split of the data. This argument can be <code>train_val_set</code>, <code>test_set</code> or <code>custom</code>.</p> </li> <li> <p><code>--download_dir &lt;download_dir&gt;</code>: Specify the path where the downloaded data will be stored.</p> </li> <li> <p><code>--download_only_one_video_sequence</code>: (Optional) Specify whether to download only one video sequence (the longest video will be downloaded). By omitting this flag, all the available video sequences for each scene will be downloaded.</p> </li> <li> <p><code>--dataset_assets &lt;identifier list of data assets to download&gt;</code>: Specify the identifier list of the data assets to download. See the table below for the supported data asset identifiers. You can specify the data assets to download as <code>--dataset_assets &lt;identifier-1&gt; &lt;identifier-2&gt; ... &lt;identifier-n&gt;</code></p> </li> </ul> <p>Below you can find a list with the supported data asset identifiers. To download the desired data assets, add the corresponding identifiers after the <code>--dataset_assets</code> argument. You can find a detailed description of each data asset at Dataset &gt; Data assets and File structure.</p> Data asset identifier Filename Description laser_scan_5mm &lt;visit_id&gt;_laser_scan.ply Combined Faro laser scan downsampled with a voxel size of 5mm crop_mask &lt;visit_id&gt;_crop_mask.npy Binary mask to crop extraneous points from the combined laser scan annotations annotations.json functional interactive element annotations descriptions descriptions.json natural language task descriptions motions motions.json motion annotations lowres_wide lowres_wide/ low res. RGB frames of the wide camera lowres_wide_intrinsics lowres_wide_intrinsics/ camera intrinsics for the low res. frames lowres_depth lowres_depth/ low res. depth maps hires_wide hires_wide/ high res. RGB frames of the wide camera hires_wide_intrinsics hires_wide_intrinsics camera intrinsics for the high res. frames hires_depth hires_depth/ high res. depth maps lowres_poses lowres_poses.traj ARKit camera poses in the ARKit coordinate system hires_poses hires_poses.traj COLMAP camera poses in the laser scan coordinate system vid_mov &lt;video_id&gt;.mov video captured with the iPad camera in .mov format vid_mp4 &lt;video_id&gt;.mp4 video captured with the iPad camera in .mp4 format arkit_mesh &lt;video_id&gt;_arkit_mesh.ply ARKit 3D mesh reconstruction of the scene transform &lt;video_id&gt;_transform.npy 4x4 transformation matrix that registers the Faro laser scan to the ARKit coordinate system"},{"location":"toolkit/data-downloader/#download-the-trainval-sets","title":"Download the train/val sets","text":"<p>To download the scenes in the train/val sets, you can run:</p> <p><pre><code>python -m data_downloader.data_asset_download --split train_val_set --download_dir data/ --dataset_assets &lt;identifier list of data assets to download&gt;\n</code></pre> where <code>&lt;identifier list of data assets to download&gt;</code> should be substituted with the identifiers of the data assets you want to download. For example, to download the combined laser scan, the high resolution RGB frames, depth maps, camera intrinsics and poses, you can run:</p> <p><pre><code>python -m data_downloader.data_asset_download --split train_val_set --download_dir data/ --dataset_assets laser_scan_5mm hires_wide hires_depth hires_wide_intrinsics hires_poses\n</code></pre> You can also add <code>--download_only_one_video_sequence</code>, if you want to download only one video sequence for each scene. This option will reduce the storage needed and the download time.</p>"},{"location":"toolkit/data-downloader/#download-the-test-set","title":"Download the test set","text":"<p>Similarly, to download the scenes in the test set, you can run:</p> <p><pre><code>python -m data_downloader.data_asset_download --split test_set --download_dir data/ --dataset_assets &lt;identifier list of data assets to download&gt;\n</code></pre> where <code>&lt;identifier list of data assets to download&gt;</code> should be substituted with the identifiers of the data assets you want to download. </p> <p>To download only one video sequence for each scene, you can add <code>--download_only_one_video_sequence</code>.</p>"},{"location":"toolkit/data-downloader/#download-selected-scenes","title":"Download selected scenes","text":"<p>Download a set of selected scenes:</p> <p>To download only a set of selected scenes, you can create a custom <code>.csv</code> file by following the same format (e.g., check here). Specifically, the first column should specify the <code>visit_id</code> and the second column the <code>video_id</code>. For example:</p> <pre><code>visit_id,video_id\n420683,42445137\n421013,42444703\n421015,42444789\n...\n</code></pre> <p>To download the data, you can run:</p> <pre><code>python -m data_downloader.data_asset_download --split custom --video_id_csv &lt;path to the .csv file&gt; --download_dir data/ --dataset_assets &lt;identifier list of data assets to download&gt;\n</code></pre> <p>The <code>video_id</code> column will be used only if the desired data assets to download are related to the video sequence.</p> <p>Download a single selected scene:</p> <p>To download a single selected scene, you can run:</p> <pre><code>python -m data_downloader.data_asset_download --split custom --download_dir data/ --visit_id &lt;visit_id of the desired scene&gt; --video_id &lt;video_id of the desired video sequence&gt; --dataset_assets &lt;identifier list of data assets to download&gt;\n</code></pre>"},{"location":"toolkit/data-parser/","title":"Data parser","text":"<p>Here, we provide the documentation for the data parser.</p>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser","title":"<code>DataParser</code>","text":"<p>A class for parsing data files in the SceneFun3D dataset.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>class DataParser:\n    \"\"\"\n    A class for parsing data files in the SceneFun3D dataset.\n    \"\"\"\n\n    def __init__(self, data_root_path):\n        \"\"\"\n        Initialize the DataParser instance with the root path.\n\n        Args:\n            data_root_path (str): The root path where data is located.\n        \"\"\"\n        self.data_root_path = os.path.join(data_root_path)\n\n    def TrajStringToMatrix(self, traj_str):\n        \"\"\" \n        Converts a line from the camera trajectory file into translation and rotation matrices.\n\n        Args:\n            traj_str (str): A space-delimited string where each line represents a camera pose at a particular timestamp. \n                            The line consists of seven columns:\n                - Column 1: timestamp\n                - Columns 2-4: rotation (axis-angle representation in radians)\n                - Columns 5-7: translation (in meters)\n\n        Returns:\n            (tuple): A tuple containing:\n                - ts (str): Timestamp.\n                - Rt (numpy.ndarray): 4x4 transformation matrix representing rotation and translation.\n\n        Raises:\n            AssertionError: If the input string does not have exactly seven columns.\n        \"\"\"\n        tokens = traj_str.split()\n        assert len(tokens) == 7\n        ts = tokens[0]\n\n        # Rotation in angle axis\n        angle_axis = [float(tokens[1]), float(tokens[2]), float(tokens[3])]\n        r_w_to_p = convert_angle_axis_to_matrix3(np.asarray(angle_axis))\n\n        # Translation\n        t_w_to_p = np.asarray([float(tokens[4]), float(tokens[5]), float(tokens[6])])\n        extrinsics = np.eye(4, 4)\n        extrinsics[:3, :3] = r_w_to_p\n        extrinsics[:3, -1] = t_w_to_p\n        Rt = np.linalg.inv(extrinsics)\n\n        return (ts, Rt)\n\n    def get_camera_trajectory(self, visit_id, video_id, pose_source=\"colmap\"):\n        \"\"\"\n        Retrieve the camera trajectory from a file and convert it into a dictionary whose keys are timestamps and \n        values are the corresponding camera poses.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n            video_id (str): The identifier of the video sequence.\n            pose_source (str, optional): Specifies the trajectory asset type, either \"colmap\" or \"arkit\". Defaults to \"colmap\".\n\n        Returns:\n            (dict): A dictionary where keys are timestamps (rounded to 3 decimal points) and values are 4x4 transformation matrices representing camera poses.\n\n        Raises:\n            AssertionError: If an unsupported trajectory asset type is provided.\n        \"\"\"\n        assert pose_source in [\"colmap\", \"arkit\"], f\"Unknown option {pose_source}\"\n\n        data_asset_identifier = \"hires_poses\" if pose_source == \"colmap\" else \"lowres_poses\"\n        traj_file_path = self.get_data_asset_path(data_asset_identifier=f\"{data_asset_identifier}\", visit_id=visit_id, video_id=video_id)\n\n        with open(traj_file_path) as f:\n            traj = f.readlines()\n\n        # Convert trajectory to a dictionary\n        poses_from_traj = {}\n        for line in traj:\n            traj_timestamp = line.split(\" \")[0]\n\n            if pose_source == \"colmap\":\n                poses_from_traj[f\"{float(traj_timestamp)}\"] = np.array(self.TrajStringToMatrix(line)[1].tolist())\n            elif pose_source == \"arkit\":\n                poses_from_traj[f\"{round(float(traj_timestamp), 3):.3f}\"] = np.array(self.TrajStringToMatrix(line)[1].tolist())\n\n        return poses_from_traj\n\n    def get_laser_scan(self, visit_id):\n        \"\"\"\n        Load a point cloud from a .ply file containing laser scan data.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n\n        Returns:\n            (open3d.geometry.PointCloud): A point cloud object containing the laser scan data (i.e., XYZRGB point cloud).\n        \"\"\"\n        laser_scan_path = self.get_data_asset_path(data_asset_identifier=\"laser_scan_5mm\", visit_id=visit_id)\n\n        pcd = o3d.io.read_point_cloud(laser_scan_path)\n\n        return pcd\n\n    def get_arkit_reconstruction(self, visit_id, video_id, format=\"point_cloud\"):\n        \"\"\"\n        Load ARKit mesh reconstruction data based on the iPad video sequence from a .ply file.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n            video_id (str): The identifier of the video sequence.\n            format (str, optional): The format of the mesh reconstruction data to load. \n                                    Supported formats are \"point_cloud\" and \"mesh\". \n                                    Defaults to \"point_cloud\".\n\n        Returns:\n            (Union[open3d.geometry.PointCloud, open3d.geometry.TriangleMesh]): \n                The loaded mesh reconstruction data in the specified format.\n\n        Raises:\n            ValueError: If an unsupported 3D data format is specified.\n        \"\"\"\n        mesh_path = self.get_data_asset_path(data_asset_identifier=\"arkit_mesh\", visit_id=visit_id, video_id=video_id)\n\n        mesh = None \n\n        if format == \"point_cloud\":\n            mesh = o3d.io.read_point_cloud(mesh_path)\n        elif format == \"mesh\":\n            mesh = o3d.io.read_triangle_mesh(mesh_path)\n        else: \n            raise ValueError(f\"Unknown mesh format {format}\")\n\n        return mesh\n\n    def get_rgb_frames(self, visit_id, video_id, data_asset_identifier=\"hires_wide\"):\n        \"\"\"\n        Retrieve the paths to the RGB frames for a given scene and video sequence.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n            video_id (str): The identifier of the video sequence.\n            data_asset_identifier (str, optional): The data asset type for the RGB frames.\n                                                   Can be either \"hires_wide\" or \"lowres_wide\". \n                                                   Defaults to \"hires_wide\".\n\n        Returns:\n            (dict): A dictionary mapping frame timestamps to their corresponding file paths.\n\n        Raises:\n            ValueError: If an unsupported data asset identifier is provided.\n            FileNotFoundError: If no frames are found at the specified path.\n        \"\"\"\n        frame_mapping = {}\n        if data_asset_identifier == \"hires_wide\":\n            rgb_frames_path = self.get_data_asset_path(data_asset_identifier=\"hires_wide\", visit_id=visit_id, video_id=video_id)\n\n            frames = sorted(glob.glob(os.path.join(rgb_frames_path, \"*.jpg\")))\n            if not frames:\n                raise FileNotFoundError(f\"No RGB frames found in {rgb_frames_path}\")\n            frame_timestamps = [os.path.basename(x).split(\".jpg\")[0].split(\"_\")[1] for x in frames]\n\n        elif data_asset_identifier == \"lowres_wide\":\n            rgb_frames_path = self.get_data_asset_path(data_asset_identifier=\"lowres_wide\", visit_id=visit_id, video_id=video_id)\n\n            frames = sorted(glob.glob(os.path.join(rgb_frames_path, \"*.png\")))\n            if not frames:\n                raise FileNotFoundError(f\"No RGB frames found in {rgb_frames_path}\")\n            frame_timestamps = [os.path.basename(x).split(\".png\")[0].split(\"_\")[1] for x in frames]\n        else: \n            raise ValueError(f\"Unknown data_asset_identifier {data_asset_identifier} for RGB frames\")\n\n        # Create mapping from timestamp to full path\n        frame_mapping = {timestamp: frame for timestamp, frame in zip(frame_timestamps, frames)}\n\n        return frame_mapping\n\n    def get_depth_frames(self, visit_id, video_id, data_asset_identifier=\"hires_depth\"):\n        \"\"\"\n        Retrieve the paths to the depth frames for a given scene and video sequence.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n            video_id (str): The identifier of the video sequence.\n            data_asset_identifier (str, optional): The data asset type for the depth frames.\n                                                   Can be either \"hires_depth\" or \"lowres_depth\". \n                                                   Defaults to \"hires_depth\".\n\n        Returns:\n            (dict): A dictionary mapping frame timestamps to their corresponding file paths.\n\n        Raises:\n            ValueError: If an unsupported data asset identifier is provided.\n            FileNotFoundError: If no depth frames are found at the specified path.\n        \"\"\"\n        frame_mapping = {}\n        if data_asset_identifier == \"hires_depth\":\n            depth_frames_path = self.get_data_asset_path(data_asset_identifier=\"hires_depth\", visit_id=visit_id, video_id=video_id)\n\n        elif data_asset_identifier == \"lowres_depth\":\n            depth_frames_path = self.get_data_asset_path(data_asset_identifier=\"lowres_depth\", visit_id=visit_id, video_id=video_id)\n\n        else: \n            raise ValueError(f\"Unknown data_asset_identifier {data_asset_identifier} for depth frames\")\n\n        frames = sorted(glob.glob(os.path.join(depth_frames_path, \"*.png\")))\n        if not frames:\n            raise FileNotFoundError(f\"No depth frames found in {depth_frames_path}\")\n        frame_timestamps = [os.path.basename(x).split(\".png\")[0].split(\"_\")[1] for x in frames]\n\n         # Create mapping from timestamp to full path\n        frame_mapping = {timestamp: frame for timestamp, frame in zip(frame_timestamps, frames)}\n\n        return frame_mapping\n\n    def get_camera_intrinsics(self, visit_id, video_id, data_asset_identifier=\"hires_wide_intrinsics\"):\n        \"\"\"\n        Retrieve the camera intrinsics for a given scene and video sequence.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n            video_id (str): The identifier of the video sequence.\n            data_asset_identifier (str, optional): The data asset type for camera intrinsics.\n                                                   Can be either \"hires_wide_intrinsics\" or \"lowres_wide_intrinsics\". \n                                                   Defaults to \"hires_wide_intrinsics\".\n\n        Returns:\n            (dict): A dictionary mapping timestamps to file paths of camera intrinsics data.\n\n        Raises:\n            ValueError: If an unsupported data asset identifier is provided.\n            FileNotFoundError: If no intrinsics files are found at the specified path.\n        \"\"\"\n        intrinsics_mapping = {}\n        if data_asset_identifier == \"hires_wide_intrinsics\":\n            intrinsics_path = self.get_data_asset_path(data_asset_identifier=\"hires_wide_intrinsics\", visit_id=visit_id, video_id=video_id)\n\n        elif data_asset_identifier == \"lowres_wide_intrinsics\":\n            intrinsics_path = self.get_data_asset_path(data_asset_identifier=\"lowres_wide_intrinsics\", visit_id=visit_id, video_id=video_id)\n\n        else: \n            raise ValueError(f\"Unknown data_asset_identifier {data_asset_identifier} for camera intrinsics\")\n\n        intrinsics = sorted(glob.glob(os.path.join(intrinsics_path, \"*.pincam\")))\n\n        if not intrinsics:\n            raise FileNotFoundError(f\"No camera intrinsics found in {intrinsics_path}\")\n\n        intrinsics_timestamps = [os.path.basename(x).split(\".pincam\")[0].split(\"_\")[1] for x in intrinsics]\n\n        # Create mapping from timestamp to full path\n        intrinsics_mapping = {timestamp: cur_intrinsics for timestamp, cur_intrinsics in zip(intrinsics_timestamps, intrinsics)}\n\n        return intrinsics_mapping\n\n    def get_nearest_pose(self, \n                            desired_timestamp,\n                            poses_from_traj, \n                            time_distance_threshold = np.inf):\n        \"\"\"\n        Get the nearest pose to a desired timestamp from a dictionary of poses.\n\n        Args:\n            desired_timestamp (str): The timestamp of the desired pose.\n            poses_from_traj (dict): A dictionary where keys are timestamps (as strings) \n                                    and values are 4x4 transformation matrices representing poses.\n            time_distance_threshold (float, optional): The maximum allowable time difference \n                                                    between the desired timestamp and the nearest pose timestamp. Defaults to np.inf.\n\n        Returns:\n            (Union[numpy.ndarray, None]): The nearest pose as a 4x4 transformation matrix if found within the specified threshold, else None.\n\n        Note:\n            The function will return the pose closest to the desired timestamp if it exists in the provided poses.\n            If the closest pose is further away than the specified `time_distance_threshold`, the function returns `None`.\n        \"\"\"\n        max_pose_timestamp = max(float(key) for key in poses_from_traj.keys())\n        min_pose_timestamp = min(float(key) for key in poses_from_traj.keys()) \n\n        if float(desired_timestamp) &lt; min_pose_timestamp or \\\n            float(desired_timestamp) &gt; max_pose_timestamp:\n            return None\n\n        if desired_timestamp in poses_from_traj.keys():\n            H = poses_from_traj[desired_timestamp]\n        else:\n            closest_timestamp = min(\n                poses_from_traj.keys(), \n                key=lambda x: abs(float(x) - float(desired_timestamp))\n            )\n\n            if abs(float(closest_timestamp) - float(desired_timestamp)) &gt; time_distance_threshold:\n                return None\n\n            H = poses_from_traj[closest_timestamp]\n\n        desired_pose = H\n\n        assert desired_pose.shape == (4, 4)\n\n        return desired_pose\n\n\n\n    def get_interpolated_pose(self, \n                                desired_timestamp,\n                                poses_from_traj, \n                                time_distance_threshold = np.inf,\n                                interpolation_method = 'split',\n                                frame_distance_threshold = np.inf):\n        \"\"\"\n        Get the interpolated pose for a desired timestamp from a dictionary of poses.\n\n        Args:\n            desired_timestamp (str): The timestamp of the desired pose.\n            poses_from_traj (dict): A dictionary where keys are timestamps (as strings) \n                                    and values are 4x4 transformation matrices representing poses.\n            time_distance_threshold (float, optional): The maximum allowable time difference \n                                                    between the desired timestamp and the nearest pose timestamps. Defaults to np.inf.\n            interpolation_method (str, optional): Method used for interpolation. Defaults to 'split'.\n                - \"split\": Performs rigid body motion interpolation in SO(3) x R^3.\n                - \"geodesic_path\": Performs rigid body motion interpolation in SE(3).\n            frame_distance_threshold (float, optional): Maximum allowable frame distance between two consecutive poses. Defaults to np.inf.\n\n        Returns:\n            (Union[numpy.ndarray, None]): The interpolated pose as a 4x4 transformation matrix, or None if not found within thresholds.\n\n        Raises:\n            ValueError: If an unsupported interpolation method is specified.\n\n        Note:\n            This function uses interpolation between two nearest poses if `desired_timestamp` is not directly available.\n            The interpolation method can be either \"split\" (for rigid body interpolation in SO(3) x R^3) or \"geodesic_path\" (for SE(3)).\n            If the difference between the timestamps or poses is beyond the specified thresholds, the function will return None.\n        \"\"\"\n\n        max_pose_timestamp = max(float(key) for key in poses_from_traj.keys())\n        min_pose_timestamp = min(float(key) for key in poses_from_traj.keys()) \n\n        if float(desired_timestamp) &lt; min_pose_timestamp or \\\n            float(desired_timestamp) &gt; max_pose_timestamp:\n            return None\n\n        if desired_timestamp in poses_from_traj.keys():\n            H = poses_from_traj[desired_timestamp]\n        else:\n            greater_closest_timestamp = min(\n                [x for x in poses_from_traj.keys() if float(x) &gt; float(desired_timestamp) ], \n                key=lambda x: abs(float(x) - float(desired_timestamp))\n            )\n            smaller_closest_timestamp = min(\n                [x for x in poses_from_traj.keys() if float(x) &lt; float(desired_timestamp) ], \n                key=lambda x: abs(float(x) - float(desired_timestamp))\n            )\n\n            if abs(float(greater_closest_timestamp) - float(desired_timestamp)) &gt; time_distance_threshold or \\\n                abs(float(smaller_closest_timestamp) - float(desired_timestamp)) &gt; time_distance_threshold:\n                return None\n\n            H0 = poses_from_traj[smaller_closest_timestamp]\n            H1 = poses_from_traj[greater_closest_timestamp]\n            H0_t = hm.trans(H0)\n            H1_t = hm.trans(H1)\n\n            if np.linalg.norm(H0_t - H1_t) &gt; frame_distance_threshold:\n                return None\n\n            if interpolation_method == \"split\":\n                H = rigid_interp_split(\n                    float(desired_timestamp), \n                    poses_from_traj[smaller_closest_timestamp], \n                    float(smaller_closest_timestamp), \n                    poses_from_traj[greater_closest_timestamp], \n                    float(greater_closest_timestamp)\n                )\n            elif interpolation_method == \"geodesic_path\":\n                H = rigid_interp_geodesic(\n                    float(desired_timestamp), \n                    poses_from_traj[smaller_closest_timestamp], \n                    float(smaller_closest_timestamp), \n                    poses_from_traj[greater_closest_timestamp], \n                    float(greater_closest_timestamp)\n                )\n            else:\n                raise ValueError(f\"Unknown interpolation method {interpolation_method}\")\n\n        desired_pose = H\n\n        assert desired_pose.shape == (4, 4)\n\n        return desired_pose\n\n\n    def get_transform(self, visit_id, video_id):\n        \"\"\"\n        Load the transformation matrix from a .npy file. This transformation matrix converts coordinates from the Faro laser scan coordinate system to the ARKit coodinate system.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n            video_id (str): The identifier of the video sequence.\n\n        Returns:\n            (numpy.ndarray): The estimated transformation matrix loaded from the file.\n        \"\"\"\n        transform_path = self.get_data_asset_path(data_asset_identifier=\"transform\", visit_id=visit_id, video_id=video_id)\n        transform = np.load(transform_path) \n        return transform\n\n\n    def read_rgb_frame(self, rgb_frame_path, normalize=False):\n        \"\"\"\n        Read an RGB frame from the specified path.\n\n        Args:\n            rgb_frame_path (str): The full path to the RGB frame file.\n            normalize (bool, optional): Whether to normalize the pixel values to the range [0, 1]. Defaults to False.\n\n        Returns:\n            (numpy.ndarray): The RGB frame as a NumPy array with the RGB color values.\n\n        \"\"\"\n        color = imageio.v2.imread(rgb_frame_path)\n\n        if normalize:\n            color = color / 255.\n\n        return color\n\n    def read_depth_frame(self, depth_frame_path, conversion_factor=1000):\n        \"\"\"\n        Read a depth frame from the specified path and convert it to depth values.\n\n        Args:\n            depth_frame_path (str): The full path to the depth frame file.\n            conversion_factor (float, optional): The conversion factor to convert pixel values to depth values. Defaults to 1000 to convert millimeters to meters.\n\n        Returns:\n            (numpy.ndarray): The depth frame as a NumPy array with the depth values.\n        \"\"\"\n\n        depth = imageio.v2.imread(depth_frame_path) / conversion_factor\n\n        return depth\n\n    def read_camera_intrinsics(self, intrinsics_file_path, format=\"tuple\"):\n        \"\"\"\n        Parses a file containing camera intrinsic parameters and returns them in the specified format.\n\n        Args:\n            intrinsics_file_path (str): The path to the file containing camera intrinsic parameters.\n            format (str, optional): The format in which to return the camera intrinsic parameters.\n                                    Supported formats are \"tuple\" and \"matrix\". Defaults to \"tuple\".\n\n        Returns:\n            (Union[tuple, numpy.ndarray]): Camera intrinsic parameters in the specified format.\n\n                - If format is \"tuple\", returns a tuple \\\\(w, h, fx, fy, hw, hh\\\\).\n                - If format is \"matrix\", returns a 3x3 numpy array representing the camera matrix.\n\n        Raises:\n            ValueError: If an unsupported format is specified.\n        \"\"\"\n        w, h, fx, fy, hw, hh = np.loadtxt(intrinsics_file_path)\n\n        if format == \"tuple\":\n            return (w, h, fx, fy, hw, hh)\n        elif format == \"matrix\":\n            return np.asarray([[fx, 0, hw], [0, fy, hh], [0, 0, 1]])\n        else:\n            raise ValueError(f\"Unknown format {format}\")\n\n    def get_crop_mask(self, visit_id, return_indices=False):\n        \"\"\"\n        Load the crop mask from a .npy file.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n            return_indices (bool, optional): Whether to return the indices of the cropped points. Defaults to False.\n\n        Returns:\n            (numpy.ndarray): The crop mask loaded from the file. If `return_indices` is False, returns a Numpy array that is a binary mask of the indices to keep. If `return_indices` is True, returns a Numpy array containing the indices of the points to keep.\n        \"\"\"\n        # crop_mask_path = os.path.join(self.data_root_path, visit_id, f\"{visit_id}_crop_mask.npy\")\n        crop_mask_path = self.get_data_asset_path(data_asset_identifier=\"crop_mask\", visit_id=visit_id)\n        crop_mask = np.load(crop_mask_path)\n\n        if return_indices:\n            return np.where(crop_mask)[0]\n        else:\n            return crop_mask\n\n    def get_cropped_laser_scan(self, visit_id, laser_scan):\n        \"\"\"\n        Crop a laser scan using a crop mask.\n\n        Args:\n            visit_id (str): The identifier of the scene.\n            laser_scan (open3d.geometry.PointCloud): The laser scan point cloud to be cropped.\n\n        Returns:\n            (open3d.geometry.PointCloud): The cropped laser scan point cloud.\n        \"\"\"\n        filtered_idx_list = self.get_crop_mask(visit_id, return_indices=True)\n\n        laser_scan_points = np.array(laser_scan.points)\n        laser_scan_colors = np.array(laser_scan.colors)\n        laser_scan_points = laser_scan_points[filtered_idx_list]\n        laser_scan_colors = laser_scan_colors[filtered_idx_list]\n\n        cropped_laser_scan = o3d.geometry.PointCloud()\n        cropped_laser_scan.points = o3d.utility.Vector3dVector(laser_scan_points)\n        cropped_laser_scan.colors = o3d.utility.Vector3dVector(laser_scan_colors)\n\n        return cropped_laser_scan\n\n\n    def get_data_asset_path(self, data_asset_identifier, visit_id, video_id=None):\n        \"\"\"\n        Get the file path for a specified data asset.\n\n        Args:\n            data_asset_identifier (str): A string identifier for the data asset.\n            visit_id (str or int): The identifier for the visit (scene).\n            video_id (str or int, optional): The identifier for the video sequence. Required if specified data asset requires a video identifier.\n\n        Returns:\n            (Path): A Path object representing the file path to the specified data asset.\n\n        Raises:\n            AssertionError: If the `data_asset_identifier` is not valid or if `video_id` is required but not provided.\n        \"\"\"\n        assert data_asset_identifier in data_asset_to_path, f\"Data asset identifier '{data_asset_identifier}' is not valid\"\n\n        data_path = data_asset_to_path[data_asset_identifier]\n\n        if (\"&lt;video_id&gt;\" in data_path) and (video_id is None):\n            assert False, f\"video_id must be specified for the data asset identifier '{data_asset_identifier}'\"\n\n        visit_id = str(visit_id)\n\n        data_path = (\n            data_path\n                .replace(\"&lt;data_dir&gt;\", self.data_root_path)\n                .replace(\"&lt;visit_id&gt;\", visit_id)\n        )\n\n        if \"&lt;video_id&gt;\" in data_path:\n            video_id = str(video_id)\n            data_path = data_path.replace(\"&lt;video_id&gt;\", video_id)\n\n        return data_path\n\n\n    def get_annotations(self, visit_id, group_excluded_points=True):\n        \"\"\"\n        Retrieve the functionality annotations for a specified scene.\n\n        Args:\n            visit_id (str or int): The identifier for the scene.\n            group_excluded_points (bool, optional): If True, all annotations with the label \"exclude\" will be grouped together \n                                                    into a single annotation instance. Defaults to True.\n\n        Returns:\n            (list): A list of annotations, each represented as a dictionary.\n\n        \"\"\"\n        annotations_path = self.get_data_asset_path(data_asset_identifier=\"annotations\", visit_id=visit_id)\n\n        annotations_data = None\n        with open(annotations_path, 'r') as f:\n            annotations_data = json.load(f)[\"annotations\"]\n\n        if group_excluded_points:\n            # group the excluded points into a single annotation instance\n            exclude_indices_set = set()\n            first_exclude_annotation = None\n            filtered_annotation_data = []\n\n            for annotation in annotations_data:\n                if annotation[\"label\"] == \"exclude\":\n                    if first_exclude_annotation is None:\n                        first_exclude_annotation = annotation\n                    exclude_indices_set.update(annotation[\"indices\"])\n                else:\n                    filtered_annotation_data.append(annotation)\n\n            if first_exclude_annotation:\n                first_exclude_annotation[\"indices\"] = sorted(list(exclude_indices_set))\n                filtered_annotation_data.append(first_exclude_annotation)\n\n            annotations_data = filtered_annotation_data\n\n        return annotations_data\n\n\n    def get_descriptions(self, visit_id):\n        \"\"\"\n        Retrieve the natural language task descriptions for a specified scene.\n\n        Args:\n            visit_id (str or int): The identifier for the scene.\n\n        Returns:\n            (list): A list of descriptions, each represented as a dictionary.\n        \"\"\"\n        descriptions_path = self.get_data_asset_path(data_asset_identifier=\"descriptions\", visit_id=visit_id)\n\n        with open(descriptions_path, 'r') as f:\n            descriptions_data = json.load(f)[\"descriptions\"]\n\n        return descriptions_data\n\n\n    def get_motions(self, visit_id):\n        \"\"\"\n        Retrieve the motion annotations for a specified scene.\n\n        Args:\n            visit_id (str or int): The identifier for the scene.\n\n        Returns:\n            (list): A list of motions, each represented as a dictionary.\n        \"\"\"\n        motions_path = self.get_data_asset_path(data_asset_identifier=\"motions\", visit_id=visit_id)\n\n        with open(motions_path, 'r') as f:\n            motions_data = json.load(f)[\"motions\"]\n\n        return motions_data\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.TrajStringToMatrix","title":"<code>TrajStringToMatrix(traj_str)</code>","text":"<p>Converts a line from the camera trajectory file into translation and rotation matrices.</p> <p>Parameters:</p> Name Type Description Default <code>traj_str</code> <code>str</code> <p>A space-delimited string where each line represents a camera pose at a particular timestamp.              The line consists of seven columns: - Column 1: timestamp - Columns 2-4: rotation (axis-angle representation in radians) - Columns 5-7: translation (in meters)</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - ts (str): Timestamp. - Rt (numpy.ndarray): 4x4 transformation matrix representing rotation and translation.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the input string does not have exactly seven columns.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def TrajStringToMatrix(self, traj_str):\n    \"\"\" \n    Converts a line from the camera trajectory file into translation and rotation matrices.\n\n    Args:\n        traj_str (str): A space-delimited string where each line represents a camera pose at a particular timestamp. \n                        The line consists of seven columns:\n            - Column 1: timestamp\n            - Columns 2-4: rotation (axis-angle representation in radians)\n            - Columns 5-7: translation (in meters)\n\n    Returns:\n        (tuple): A tuple containing:\n            - ts (str): Timestamp.\n            - Rt (numpy.ndarray): 4x4 transformation matrix representing rotation and translation.\n\n    Raises:\n        AssertionError: If the input string does not have exactly seven columns.\n    \"\"\"\n    tokens = traj_str.split()\n    assert len(tokens) == 7\n    ts = tokens[0]\n\n    # Rotation in angle axis\n    angle_axis = [float(tokens[1]), float(tokens[2]), float(tokens[3])]\n    r_w_to_p = convert_angle_axis_to_matrix3(np.asarray(angle_axis))\n\n    # Translation\n    t_w_to_p = np.asarray([float(tokens[4]), float(tokens[5]), float(tokens[6])])\n    extrinsics = np.eye(4, 4)\n    extrinsics[:3, :3] = r_w_to_p\n    extrinsics[:3, -1] = t_w_to_p\n    Rt = np.linalg.inv(extrinsics)\n\n    return (ts, Rt)\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.__init__","title":"<code>__init__(data_root_path)</code>","text":"<p>Initialize the DataParser instance with the root path.</p> <p>Parameters:</p> Name Type Description Default <code>data_root_path</code> <code>str</code> <p>The root path where data is located.</p> required Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def __init__(self, data_root_path):\n    \"\"\"\n    Initialize the DataParser instance with the root path.\n\n    Args:\n        data_root_path (str): The root path where data is located.\n    \"\"\"\n    self.data_root_path = os.path.join(data_root_path)\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_annotations","title":"<code>get_annotations(visit_id, group_excluded_points=True)</code>","text":"<p>Retrieve the functionality annotations for a specified scene.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str or int</code> <p>The identifier for the scene.</p> required <code>group_excluded_points</code> <code>bool</code> <p>If True, all annotations with the label \"exclude\" will be grouped together                                      into a single annotation instance. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of annotations, each represented as a dictionary.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_annotations(self, visit_id, group_excluded_points=True):\n    \"\"\"\n    Retrieve the functionality annotations for a specified scene.\n\n    Args:\n        visit_id (str or int): The identifier for the scene.\n        group_excluded_points (bool, optional): If True, all annotations with the label \"exclude\" will be grouped together \n                                                into a single annotation instance. Defaults to True.\n\n    Returns:\n        (list): A list of annotations, each represented as a dictionary.\n\n    \"\"\"\n    annotations_path = self.get_data_asset_path(data_asset_identifier=\"annotations\", visit_id=visit_id)\n\n    annotations_data = None\n    with open(annotations_path, 'r') as f:\n        annotations_data = json.load(f)[\"annotations\"]\n\n    if group_excluded_points:\n        # group the excluded points into a single annotation instance\n        exclude_indices_set = set()\n        first_exclude_annotation = None\n        filtered_annotation_data = []\n\n        for annotation in annotations_data:\n            if annotation[\"label\"] == \"exclude\":\n                if first_exclude_annotation is None:\n                    first_exclude_annotation = annotation\n                exclude_indices_set.update(annotation[\"indices\"])\n            else:\n                filtered_annotation_data.append(annotation)\n\n        if first_exclude_annotation:\n            first_exclude_annotation[\"indices\"] = sorted(list(exclude_indices_set))\n            filtered_annotation_data.append(first_exclude_annotation)\n\n        annotations_data = filtered_annotation_data\n\n    return annotations_data\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_arkit_reconstruction","title":"<code>get_arkit_reconstruction(visit_id, video_id, format='point_cloud')</code>","text":"<p>Load ARKit mesh reconstruction data based on the iPad video sequence from a .ply file.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <code>video_id</code> <code>str</code> <p>The identifier of the video sequence.</p> required <code>format</code> <code>str</code> <p>The format of the mesh reconstruction data to load.                      Supported formats are \"point_cloud\" and \"mesh\".                      Defaults to \"point_cloud\".</p> <code>'point_cloud'</code> <p>Returns:</p> Type Description <code>Union[PointCloud, TriangleMesh]</code> <p>The loaded mesh reconstruction data in the specified format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported 3D data format is specified.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_arkit_reconstruction(self, visit_id, video_id, format=\"point_cloud\"):\n    \"\"\"\n    Load ARKit mesh reconstruction data based on the iPad video sequence from a .ply file.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n        video_id (str): The identifier of the video sequence.\n        format (str, optional): The format of the mesh reconstruction data to load. \n                                Supported formats are \"point_cloud\" and \"mesh\". \n                                Defaults to \"point_cloud\".\n\n    Returns:\n        (Union[open3d.geometry.PointCloud, open3d.geometry.TriangleMesh]): \n            The loaded mesh reconstruction data in the specified format.\n\n    Raises:\n        ValueError: If an unsupported 3D data format is specified.\n    \"\"\"\n    mesh_path = self.get_data_asset_path(data_asset_identifier=\"arkit_mesh\", visit_id=visit_id, video_id=video_id)\n\n    mesh = None \n\n    if format == \"point_cloud\":\n        mesh = o3d.io.read_point_cloud(mesh_path)\n    elif format == \"mesh\":\n        mesh = o3d.io.read_triangle_mesh(mesh_path)\n    else: \n        raise ValueError(f\"Unknown mesh format {format}\")\n\n    return mesh\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_camera_intrinsics","title":"<code>get_camera_intrinsics(visit_id, video_id, data_asset_identifier='hires_wide_intrinsics')</code>","text":"<p>Retrieve the camera intrinsics for a given scene and video sequence.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <code>video_id</code> <code>str</code> <p>The identifier of the video sequence.</p> required <code>data_asset_identifier</code> <code>str</code> <p>The data asset type for camera intrinsics.                                    Can be either \"hires_wide_intrinsics\" or \"lowres_wide_intrinsics\".                                     Defaults to \"hires_wide_intrinsics\".</p> <code>'hires_wide_intrinsics'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping timestamps to file paths of camera intrinsics data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported data asset identifier is provided.</p> <code>FileNotFoundError</code> <p>If no intrinsics files are found at the specified path.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_camera_intrinsics(self, visit_id, video_id, data_asset_identifier=\"hires_wide_intrinsics\"):\n    \"\"\"\n    Retrieve the camera intrinsics for a given scene and video sequence.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n        video_id (str): The identifier of the video sequence.\n        data_asset_identifier (str, optional): The data asset type for camera intrinsics.\n                                               Can be either \"hires_wide_intrinsics\" or \"lowres_wide_intrinsics\". \n                                               Defaults to \"hires_wide_intrinsics\".\n\n    Returns:\n        (dict): A dictionary mapping timestamps to file paths of camera intrinsics data.\n\n    Raises:\n        ValueError: If an unsupported data asset identifier is provided.\n        FileNotFoundError: If no intrinsics files are found at the specified path.\n    \"\"\"\n    intrinsics_mapping = {}\n    if data_asset_identifier == \"hires_wide_intrinsics\":\n        intrinsics_path = self.get_data_asset_path(data_asset_identifier=\"hires_wide_intrinsics\", visit_id=visit_id, video_id=video_id)\n\n    elif data_asset_identifier == \"lowres_wide_intrinsics\":\n        intrinsics_path = self.get_data_asset_path(data_asset_identifier=\"lowres_wide_intrinsics\", visit_id=visit_id, video_id=video_id)\n\n    else: \n        raise ValueError(f\"Unknown data_asset_identifier {data_asset_identifier} for camera intrinsics\")\n\n    intrinsics = sorted(glob.glob(os.path.join(intrinsics_path, \"*.pincam\")))\n\n    if not intrinsics:\n        raise FileNotFoundError(f\"No camera intrinsics found in {intrinsics_path}\")\n\n    intrinsics_timestamps = [os.path.basename(x).split(\".pincam\")[0].split(\"_\")[1] for x in intrinsics]\n\n    # Create mapping from timestamp to full path\n    intrinsics_mapping = {timestamp: cur_intrinsics for timestamp, cur_intrinsics in zip(intrinsics_timestamps, intrinsics)}\n\n    return intrinsics_mapping\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_camera_trajectory","title":"<code>get_camera_trajectory(visit_id, video_id, pose_source='colmap')</code>","text":"<p>Retrieve the camera trajectory from a file and convert it into a dictionary whose keys are timestamps and  values are the corresponding camera poses.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <code>video_id</code> <code>str</code> <p>The identifier of the video sequence.</p> required <code>pose_source</code> <code>str</code> <p>Specifies the trajectory asset type, either \"colmap\" or \"arkit\". Defaults to \"colmap\".</p> <code>'colmap'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where keys are timestamps (rounded to 3 decimal points) and values are 4x4 transformation matrices representing camera poses.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If an unsupported trajectory asset type is provided.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_camera_trajectory(self, visit_id, video_id, pose_source=\"colmap\"):\n    \"\"\"\n    Retrieve the camera trajectory from a file and convert it into a dictionary whose keys are timestamps and \n    values are the corresponding camera poses.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n        video_id (str): The identifier of the video sequence.\n        pose_source (str, optional): Specifies the trajectory asset type, either \"colmap\" or \"arkit\". Defaults to \"colmap\".\n\n    Returns:\n        (dict): A dictionary where keys are timestamps (rounded to 3 decimal points) and values are 4x4 transformation matrices representing camera poses.\n\n    Raises:\n        AssertionError: If an unsupported trajectory asset type is provided.\n    \"\"\"\n    assert pose_source in [\"colmap\", \"arkit\"], f\"Unknown option {pose_source}\"\n\n    data_asset_identifier = \"hires_poses\" if pose_source == \"colmap\" else \"lowres_poses\"\n    traj_file_path = self.get_data_asset_path(data_asset_identifier=f\"{data_asset_identifier}\", visit_id=visit_id, video_id=video_id)\n\n    with open(traj_file_path) as f:\n        traj = f.readlines()\n\n    # Convert trajectory to a dictionary\n    poses_from_traj = {}\n    for line in traj:\n        traj_timestamp = line.split(\" \")[0]\n\n        if pose_source == \"colmap\":\n            poses_from_traj[f\"{float(traj_timestamp)}\"] = np.array(self.TrajStringToMatrix(line)[1].tolist())\n        elif pose_source == \"arkit\":\n            poses_from_traj[f\"{round(float(traj_timestamp), 3):.3f}\"] = np.array(self.TrajStringToMatrix(line)[1].tolist())\n\n    return poses_from_traj\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_crop_mask","title":"<code>get_crop_mask(visit_id, return_indices=False)</code>","text":"<p>Load the crop mask from a .npy file.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <code>return_indices</code> <code>bool</code> <p>Whether to return the indices of the cropped points. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The crop mask loaded from the file. If <code>return_indices</code> is False, returns a Numpy array that is a binary mask of the indices to keep. If <code>return_indices</code> is True, returns a Numpy array containing the indices of the points to keep.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_crop_mask(self, visit_id, return_indices=False):\n    \"\"\"\n    Load the crop mask from a .npy file.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n        return_indices (bool, optional): Whether to return the indices of the cropped points. Defaults to False.\n\n    Returns:\n        (numpy.ndarray): The crop mask loaded from the file. If `return_indices` is False, returns a Numpy array that is a binary mask of the indices to keep. If `return_indices` is True, returns a Numpy array containing the indices of the points to keep.\n    \"\"\"\n    # crop_mask_path = os.path.join(self.data_root_path, visit_id, f\"{visit_id}_crop_mask.npy\")\n    crop_mask_path = self.get_data_asset_path(data_asset_identifier=\"crop_mask\", visit_id=visit_id)\n    crop_mask = np.load(crop_mask_path)\n\n    if return_indices:\n        return np.where(crop_mask)[0]\n    else:\n        return crop_mask\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_cropped_laser_scan","title":"<code>get_cropped_laser_scan(visit_id, laser_scan)</code>","text":"<p>Crop a laser scan using a crop mask.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <code>laser_scan</code> <code>PointCloud</code> <p>The laser scan point cloud to be cropped.</p> required <p>Returns:</p> Type Description <code>PointCloud</code> <p>The cropped laser scan point cloud.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_cropped_laser_scan(self, visit_id, laser_scan):\n    \"\"\"\n    Crop a laser scan using a crop mask.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n        laser_scan (open3d.geometry.PointCloud): The laser scan point cloud to be cropped.\n\n    Returns:\n        (open3d.geometry.PointCloud): The cropped laser scan point cloud.\n    \"\"\"\n    filtered_idx_list = self.get_crop_mask(visit_id, return_indices=True)\n\n    laser_scan_points = np.array(laser_scan.points)\n    laser_scan_colors = np.array(laser_scan.colors)\n    laser_scan_points = laser_scan_points[filtered_idx_list]\n    laser_scan_colors = laser_scan_colors[filtered_idx_list]\n\n    cropped_laser_scan = o3d.geometry.PointCloud()\n    cropped_laser_scan.points = o3d.utility.Vector3dVector(laser_scan_points)\n    cropped_laser_scan.colors = o3d.utility.Vector3dVector(laser_scan_colors)\n\n    return cropped_laser_scan\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_data_asset_path","title":"<code>get_data_asset_path(data_asset_identifier, visit_id, video_id=None)</code>","text":"<p>Get the file path for a specified data asset.</p> <p>Parameters:</p> Name Type Description Default <code>data_asset_identifier</code> <code>str</code> <p>A string identifier for the data asset.</p> required <code>visit_id</code> <code>str or int</code> <p>The identifier for the visit (scene).</p> required <code>video_id</code> <code>str or int</code> <p>The identifier for the video sequence. Required if specified data asset requires a video identifier.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object representing the file path to the specified data asset.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the <code>data_asset_identifier</code> is not valid or if <code>video_id</code> is required but not provided.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_data_asset_path(self, data_asset_identifier, visit_id, video_id=None):\n    \"\"\"\n    Get the file path for a specified data asset.\n\n    Args:\n        data_asset_identifier (str): A string identifier for the data asset.\n        visit_id (str or int): The identifier for the visit (scene).\n        video_id (str or int, optional): The identifier for the video sequence. Required if specified data asset requires a video identifier.\n\n    Returns:\n        (Path): A Path object representing the file path to the specified data asset.\n\n    Raises:\n        AssertionError: If the `data_asset_identifier` is not valid or if `video_id` is required but not provided.\n    \"\"\"\n    assert data_asset_identifier in data_asset_to_path, f\"Data asset identifier '{data_asset_identifier}' is not valid\"\n\n    data_path = data_asset_to_path[data_asset_identifier]\n\n    if (\"&lt;video_id&gt;\" in data_path) and (video_id is None):\n        assert False, f\"video_id must be specified for the data asset identifier '{data_asset_identifier}'\"\n\n    visit_id = str(visit_id)\n\n    data_path = (\n        data_path\n            .replace(\"&lt;data_dir&gt;\", self.data_root_path)\n            .replace(\"&lt;visit_id&gt;\", visit_id)\n    )\n\n    if \"&lt;video_id&gt;\" in data_path:\n        video_id = str(video_id)\n        data_path = data_path.replace(\"&lt;video_id&gt;\", video_id)\n\n    return data_path\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_depth_frames","title":"<code>get_depth_frames(visit_id, video_id, data_asset_identifier='hires_depth')</code>","text":"<p>Retrieve the paths to the depth frames for a given scene and video sequence.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <code>video_id</code> <code>str</code> <p>The identifier of the video sequence.</p> required <code>data_asset_identifier</code> <code>str</code> <p>The data asset type for the depth frames.                                    Can be either \"hires_depth\" or \"lowres_depth\".                                     Defaults to \"hires_depth\".</p> <code>'hires_depth'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping frame timestamps to their corresponding file paths.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported data asset identifier is provided.</p> <code>FileNotFoundError</code> <p>If no depth frames are found at the specified path.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_depth_frames(self, visit_id, video_id, data_asset_identifier=\"hires_depth\"):\n    \"\"\"\n    Retrieve the paths to the depth frames for a given scene and video sequence.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n        video_id (str): The identifier of the video sequence.\n        data_asset_identifier (str, optional): The data asset type for the depth frames.\n                                               Can be either \"hires_depth\" or \"lowres_depth\". \n                                               Defaults to \"hires_depth\".\n\n    Returns:\n        (dict): A dictionary mapping frame timestamps to their corresponding file paths.\n\n    Raises:\n        ValueError: If an unsupported data asset identifier is provided.\n        FileNotFoundError: If no depth frames are found at the specified path.\n    \"\"\"\n    frame_mapping = {}\n    if data_asset_identifier == \"hires_depth\":\n        depth_frames_path = self.get_data_asset_path(data_asset_identifier=\"hires_depth\", visit_id=visit_id, video_id=video_id)\n\n    elif data_asset_identifier == \"lowres_depth\":\n        depth_frames_path = self.get_data_asset_path(data_asset_identifier=\"lowres_depth\", visit_id=visit_id, video_id=video_id)\n\n    else: \n        raise ValueError(f\"Unknown data_asset_identifier {data_asset_identifier} for depth frames\")\n\n    frames = sorted(glob.glob(os.path.join(depth_frames_path, \"*.png\")))\n    if not frames:\n        raise FileNotFoundError(f\"No depth frames found in {depth_frames_path}\")\n    frame_timestamps = [os.path.basename(x).split(\".png\")[0].split(\"_\")[1] for x in frames]\n\n     # Create mapping from timestamp to full path\n    frame_mapping = {timestamp: frame for timestamp, frame in zip(frame_timestamps, frames)}\n\n    return frame_mapping\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_descriptions","title":"<code>get_descriptions(visit_id)</code>","text":"<p>Retrieve the natural language task descriptions for a specified scene.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str or int</code> <p>The identifier for the scene.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of descriptions, each represented as a dictionary.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_descriptions(self, visit_id):\n    \"\"\"\n    Retrieve the natural language task descriptions for a specified scene.\n\n    Args:\n        visit_id (str or int): The identifier for the scene.\n\n    Returns:\n        (list): A list of descriptions, each represented as a dictionary.\n    \"\"\"\n    descriptions_path = self.get_data_asset_path(data_asset_identifier=\"descriptions\", visit_id=visit_id)\n\n    with open(descriptions_path, 'r') as f:\n        descriptions_data = json.load(f)[\"descriptions\"]\n\n    return descriptions_data\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_interpolated_pose","title":"<code>get_interpolated_pose(desired_timestamp, poses_from_traj, time_distance_threshold=np.inf, interpolation_method='split', frame_distance_threshold=np.inf)</code>","text":"<p>Get the interpolated pose for a desired timestamp from a dictionary of poses.</p> <p>Parameters:</p> Name Type Description Default <code>desired_timestamp</code> <code>str</code> <p>The timestamp of the desired pose.</p> required <code>poses_from_traj</code> <code>dict</code> <p>A dictionary where keys are timestamps (as strings)                      and values are 4x4 transformation matrices representing poses.</p> required <code>time_distance_threshold</code> <code>float</code> <p>The maximum allowable time difference                                      between the desired timestamp and the nearest pose timestamps. Defaults to np.inf.</p> <code>inf</code> <code>interpolation_method</code> <code>str</code> <p>Method used for interpolation. Defaults to 'split'. - \"split\": Performs rigid body motion interpolation in SO(3) x R^3. - \"geodesic_path\": Performs rigid body motion interpolation in SE(3).</p> <code>'split'</code> <code>frame_distance_threshold</code> <code>float</code> <p>Maximum allowable frame distance between two consecutive poses. Defaults to np.inf.</p> <code>inf</code> <p>Returns:</p> Type Description <code>Union[ndarray, None]</code> <p>The interpolated pose as a 4x4 transformation matrix, or None if not found within thresholds.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported interpolation method is specified.</p> Note <p>This function uses interpolation between two nearest poses if <code>desired_timestamp</code> is not directly available. The interpolation method can be either \"split\" (for rigid body interpolation in SO(3) x R^3) or \"geodesic_path\" (for SE(3)). If the difference between the timestamps or poses is beyond the specified thresholds, the function will return None.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_interpolated_pose(self, \n                            desired_timestamp,\n                            poses_from_traj, \n                            time_distance_threshold = np.inf,\n                            interpolation_method = 'split',\n                            frame_distance_threshold = np.inf):\n    \"\"\"\n    Get the interpolated pose for a desired timestamp from a dictionary of poses.\n\n    Args:\n        desired_timestamp (str): The timestamp of the desired pose.\n        poses_from_traj (dict): A dictionary where keys are timestamps (as strings) \n                                and values are 4x4 transformation matrices representing poses.\n        time_distance_threshold (float, optional): The maximum allowable time difference \n                                                between the desired timestamp and the nearest pose timestamps. Defaults to np.inf.\n        interpolation_method (str, optional): Method used for interpolation. Defaults to 'split'.\n            - \"split\": Performs rigid body motion interpolation in SO(3) x R^3.\n            - \"geodesic_path\": Performs rigid body motion interpolation in SE(3).\n        frame_distance_threshold (float, optional): Maximum allowable frame distance between two consecutive poses. Defaults to np.inf.\n\n    Returns:\n        (Union[numpy.ndarray, None]): The interpolated pose as a 4x4 transformation matrix, or None if not found within thresholds.\n\n    Raises:\n        ValueError: If an unsupported interpolation method is specified.\n\n    Note:\n        This function uses interpolation between two nearest poses if `desired_timestamp` is not directly available.\n        The interpolation method can be either \"split\" (for rigid body interpolation in SO(3) x R^3) or \"geodesic_path\" (for SE(3)).\n        If the difference between the timestamps or poses is beyond the specified thresholds, the function will return None.\n    \"\"\"\n\n    max_pose_timestamp = max(float(key) for key in poses_from_traj.keys())\n    min_pose_timestamp = min(float(key) for key in poses_from_traj.keys()) \n\n    if float(desired_timestamp) &lt; min_pose_timestamp or \\\n        float(desired_timestamp) &gt; max_pose_timestamp:\n        return None\n\n    if desired_timestamp in poses_from_traj.keys():\n        H = poses_from_traj[desired_timestamp]\n    else:\n        greater_closest_timestamp = min(\n            [x for x in poses_from_traj.keys() if float(x) &gt; float(desired_timestamp) ], \n            key=lambda x: abs(float(x) - float(desired_timestamp))\n        )\n        smaller_closest_timestamp = min(\n            [x for x in poses_from_traj.keys() if float(x) &lt; float(desired_timestamp) ], \n            key=lambda x: abs(float(x) - float(desired_timestamp))\n        )\n\n        if abs(float(greater_closest_timestamp) - float(desired_timestamp)) &gt; time_distance_threshold or \\\n            abs(float(smaller_closest_timestamp) - float(desired_timestamp)) &gt; time_distance_threshold:\n            return None\n\n        H0 = poses_from_traj[smaller_closest_timestamp]\n        H1 = poses_from_traj[greater_closest_timestamp]\n        H0_t = hm.trans(H0)\n        H1_t = hm.trans(H1)\n\n        if np.linalg.norm(H0_t - H1_t) &gt; frame_distance_threshold:\n            return None\n\n        if interpolation_method == \"split\":\n            H = rigid_interp_split(\n                float(desired_timestamp), \n                poses_from_traj[smaller_closest_timestamp], \n                float(smaller_closest_timestamp), \n                poses_from_traj[greater_closest_timestamp], \n                float(greater_closest_timestamp)\n            )\n        elif interpolation_method == \"geodesic_path\":\n            H = rigid_interp_geodesic(\n                float(desired_timestamp), \n                poses_from_traj[smaller_closest_timestamp], \n                float(smaller_closest_timestamp), \n                poses_from_traj[greater_closest_timestamp], \n                float(greater_closest_timestamp)\n            )\n        else:\n            raise ValueError(f\"Unknown interpolation method {interpolation_method}\")\n\n    desired_pose = H\n\n    assert desired_pose.shape == (4, 4)\n\n    return desired_pose\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_laser_scan","title":"<code>get_laser_scan(visit_id)</code>","text":"<p>Load a point cloud from a .ply file containing laser scan data.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <p>Returns:</p> Type Description <code>PointCloud</code> <p>A point cloud object containing the laser scan data (i.e., XYZRGB point cloud).</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_laser_scan(self, visit_id):\n    \"\"\"\n    Load a point cloud from a .ply file containing laser scan data.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n\n    Returns:\n        (open3d.geometry.PointCloud): A point cloud object containing the laser scan data (i.e., XYZRGB point cloud).\n    \"\"\"\n    laser_scan_path = self.get_data_asset_path(data_asset_identifier=\"laser_scan_5mm\", visit_id=visit_id)\n\n    pcd = o3d.io.read_point_cloud(laser_scan_path)\n\n    return pcd\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_motions","title":"<code>get_motions(visit_id)</code>","text":"<p>Retrieve the motion annotations for a specified scene.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str or int</code> <p>The identifier for the scene.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of motions, each represented as a dictionary.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_motions(self, visit_id):\n    \"\"\"\n    Retrieve the motion annotations for a specified scene.\n\n    Args:\n        visit_id (str or int): The identifier for the scene.\n\n    Returns:\n        (list): A list of motions, each represented as a dictionary.\n    \"\"\"\n    motions_path = self.get_data_asset_path(data_asset_identifier=\"motions\", visit_id=visit_id)\n\n    with open(motions_path, 'r') as f:\n        motions_data = json.load(f)[\"motions\"]\n\n    return motions_data\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_nearest_pose","title":"<code>get_nearest_pose(desired_timestamp, poses_from_traj, time_distance_threshold=np.inf)</code>","text":"<p>Get the nearest pose to a desired timestamp from a dictionary of poses.</p> <p>Parameters:</p> Name Type Description Default <code>desired_timestamp</code> <code>str</code> <p>The timestamp of the desired pose.</p> required <code>poses_from_traj</code> <code>dict</code> <p>A dictionary where keys are timestamps (as strings)                      and values are 4x4 transformation matrices representing poses.</p> required <code>time_distance_threshold</code> <code>float</code> <p>The maximum allowable time difference                                      between the desired timestamp and the nearest pose timestamp. Defaults to np.inf.</p> <code>inf</code> <p>Returns:</p> Type Description <code>Union[ndarray, None]</code> <p>The nearest pose as a 4x4 transformation matrix if found within the specified threshold, else None.</p> Note <p>The function will return the pose closest to the desired timestamp if it exists in the provided poses. If the closest pose is further away than the specified <code>time_distance_threshold</code>, the function returns <code>None</code>.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_nearest_pose(self, \n                        desired_timestamp,\n                        poses_from_traj, \n                        time_distance_threshold = np.inf):\n    \"\"\"\n    Get the nearest pose to a desired timestamp from a dictionary of poses.\n\n    Args:\n        desired_timestamp (str): The timestamp of the desired pose.\n        poses_from_traj (dict): A dictionary where keys are timestamps (as strings) \n                                and values are 4x4 transformation matrices representing poses.\n        time_distance_threshold (float, optional): The maximum allowable time difference \n                                                between the desired timestamp and the nearest pose timestamp. Defaults to np.inf.\n\n    Returns:\n        (Union[numpy.ndarray, None]): The nearest pose as a 4x4 transformation matrix if found within the specified threshold, else None.\n\n    Note:\n        The function will return the pose closest to the desired timestamp if it exists in the provided poses.\n        If the closest pose is further away than the specified `time_distance_threshold`, the function returns `None`.\n    \"\"\"\n    max_pose_timestamp = max(float(key) for key in poses_from_traj.keys())\n    min_pose_timestamp = min(float(key) for key in poses_from_traj.keys()) \n\n    if float(desired_timestamp) &lt; min_pose_timestamp or \\\n        float(desired_timestamp) &gt; max_pose_timestamp:\n        return None\n\n    if desired_timestamp in poses_from_traj.keys():\n        H = poses_from_traj[desired_timestamp]\n    else:\n        closest_timestamp = min(\n            poses_from_traj.keys(), \n            key=lambda x: abs(float(x) - float(desired_timestamp))\n        )\n\n        if abs(float(closest_timestamp) - float(desired_timestamp)) &gt; time_distance_threshold:\n            return None\n\n        H = poses_from_traj[closest_timestamp]\n\n    desired_pose = H\n\n    assert desired_pose.shape == (4, 4)\n\n    return desired_pose\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_rgb_frames","title":"<code>get_rgb_frames(visit_id, video_id, data_asset_identifier='hires_wide')</code>","text":"<p>Retrieve the paths to the RGB frames for a given scene and video sequence.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <code>video_id</code> <code>str</code> <p>The identifier of the video sequence.</p> required <code>data_asset_identifier</code> <code>str</code> <p>The data asset type for the RGB frames.                                    Can be either \"hires_wide\" or \"lowres_wide\".                                     Defaults to \"hires_wide\".</p> <code>'hires_wide'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping frame timestamps to their corresponding file paths.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported data asset identifier is provided.</p> <code>FileNotFoundError</code> <p>If no frames are found at the specified path.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_rgb_frames(self, visit_id, video_id, data_asset_identifier=\"hires_wide\"):\n    \"\"\"\n    Retrieve the paths to the RGB frames for a given scene and video sequence.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n        video_id (str): The identifier of the video sequence.\n        data_asset_identifier (str, optional): The data asset type for the RGB frames.\n                                               Can be either \"hires_wide\" or \"lowres_wide\". \n                                               Defaults to \"hires_wide\".\n\n    Returns:\n        (dict): A dictionary mapping frame timestamps to their corresponding file paths.\n\n    Raises:\n        ValueError: If an unsupported data asset identifier is provided.\n        FileNotFoundError: If no frames are found at the specified path.\n    \"\"\"\n    frame_mapping = {}\n    if data_asset_identifier == \"hires_wide\":\n        rgb_frames_path = self.get_data_asset_path(data_asset_identifier=\"hires_wide\", visit_id=visit_id, video_id=video_id)\n\n        frames = sorted(glob.glob(os.path.join(rgb_frames_path, \"*.jpg\")))\n        if not frames:\n            raise FileNotFoundError(f\"No RGB frames found in {rgb_frames_path}\")\n        frame_timestamps = [os.path.basename(x).split(\".jpg\")[0].split(\"_\")[1] for x in frames]\n\n    elif data_asset_identifier == \"lowres_wide\":\n        rgb_frames_path = self.get_data_asset_path(data_asset_identifier=\"lowres_wide\", visit_id=visit_id, video_id=video_id)\n\n        frames = sorted(glob.glob(os.path.join(rgb_frames_path, \"*.png\")))\n        if not frames:\n            raise FileNotFoundError(f\"No RGB frames found in {rgb_frames_path}\")\n        frame_timestamps = [os.path.basename(x).split(\".png\")[0].split(\"_\")[1] for x in frames]\n    else: \n        raise ValueError(f\"Unknown data_asset_identifier {data_asset_identifier} for RGB frames\")\n\n    # Create mapping from timestamp to full path\n    frame_mapping = {timestamp: frame for timestamp, frame in zip(frame_timestamps, frames)}\n\n    return frame_mapping\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.get_transform","title":"<code>get_transform(visit_id, video_id)</code>","text":"<p>Load the transformation matrix from a .npy file. This transformation matrix converts coordinates from the Faro laser scan coordinate system to the ARKit coodinate system.</p> <p>Parameters:</p> Name Type Description Default <code>visit_id</code> <code>str</code> <p>The identifier of the scene.</p> required <code>video_id</code> <code>str</code> <p>The identifier of the video sequence.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The estimated transformation matrix loaded from the file.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def get_transform(self, visit_id, video_id):\n    \"\"\"\n    Load the transformation matrix from a .npy file. This transformation matrix converts coordinates from the Faro laser scan coordinate system to the ARKit coodinate system.\n\n    Args:\n        visit_id (str): The identifier of the scene.\n        video_id (str): The identifier of the video sequence.\n\n    Returns:\n        (numpy.ndarray): The estimated transformation matrix loaded from the file.\n    \"\"\"\n    transform_path = self.get_data_asset_path(data_asset_identifier=\"transform\", visit_id=visit_id, video_id=video_id)\n    transform = np.load(transform_path) \n    return transform\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.read_camera_intrinsics","title":"<code>read_camera_intrinsics(intrinsics_file_path, format='tuple')</code>","text":"<p>Parses a file containing camera intrinsic parameters and returns them in the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>intrinsics_file_path</code> <code>str</code> <p>The path to the file containing camera intrinsic parameters.</p> required <code>format</code> <code>str</code> <p>The format in which to return the camera intrinsic parameters.                     Supported formats are \"tuple\" and \"matrix\". Defaults to \"tuple\".</p> <code>'tuple'</code> <p>Returns:</p> Type Description <code>Union[tuple, ndarray]</code> <p>Camera intrinsic parameters in the specified format.</p> <ul> <li>If format is \"tuple\", returns a tuple \\(w, h, fx, fy, hw, hh\\).</li> <li>If format is \"matrix\", returns a 3x3 numpy array representing the camera matrix.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported format is specified.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def read_camera_intrinsics(self, intrinsics_file_path, format=\"tuple\"):\n    \"\"\"\n    Parses a file containing camera intrinsic parameters and returns them in the specified format.\n\n    Args:\n        intrinsics_file_path (str): The path to the file containing camera intrinsic parameters.\n        format (str, optional): The format in which to return the camera intrinsic parameters.\n                                Supported formats are \"tuple\" and \"matrix\". Defaults to \"tuple\".\n\n    Returns:\n        (Union[tuple, numpy.ndarray]): Camera intrinsic parameters in the specified format.\n\n            - If format is \"tuple\", returns a tuple \\\\(w, h, fx, fy, hw, hh\\\\).\n            - If format is \"matrix\", returns a 3x3 numpy array representing the camera matrix.\n\n    Raises:\n        ValueError: If an unsupported format is specified.\n    \"\"\"\n    w, h, fx, fy, hw, hh = np.loadtxt(intrinsics_file_path)\n\n    if format == \"tuple\":\n        return (w, h, fx, fy, hw, hh)\n    elif format == \"matrix\":\n        return np.asarray([[fx, 0, hw], [0, fy, hh], [0, 0, 1]])\n    else:\n        raise ValueError(f\"Unknown format {format}\")\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.read_depth_frame","title":"<code>read_depth_frame(depth_frame_path, conversion_factor=1000)</code>","text":"<p>Read a depth frame from the specified path and convert it to depth values.</p> <p>Parameters:</p> Name Type Description Default <code>depth_frame_path</code> <code>str</code> <p>The full path to the depth frame file.</p> required <code>conversion_factor</code> <code>float</code> <p>The conversion factor to convert pixel values to depth values. Defaults to 1000 to convert millimeters to meters.</p> <code>1000</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The depth frame as a NumPy array with the depth values.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def read_depth_frame(self, depth_frame_path, conversion_factor=1000):\n    \"\"\"\n    Read a depth frame from the specified path and convert it to depth values.\n\n    Args:\n        depth_frame_path (str): The full path to the depth frame file.\n        conversion_factor (float, optional): The conversion factor to convert pixel values to depth values. Defaults to 1000 to convert millimeters to meters.\n\n    Returns:\n        (numpy.ndarray): The depth frame as a NumPy array with the depth values.\n    \"\"\"\n\n    depth = imageio.v2.imread(depth_frame_path) / conversion_factor\n\n    return depth\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.DataParser.read_rgb_frame","title":"<code>read_rgb_frame(rgb_frame_path, normalize=False)</code>","text":"<p>Read an RGB frame from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>rgb_frame_path</code> <code>str</code> <p>The full path to the RGB frame file.</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize the pixel values to the range [0, 1]. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The RGB frame as a NumPy array with the RGB color values.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def read_rgb_frame(self, rgb_frame_path, normalize=False):\n    \"\"\"\n    Read an RGB frame from the specified path.\n\n    Args:\n        rgb_frame_path (str): The full path to the RGB frame file.\n        normalize (bool, optional): Whether to normalize the pixel values to the range [0, 1]. Defaults to False.\n\n    Returns:\n        (numpy.ndarray): The RGB frame as a NumPy array with the RGB color values.\n\n    \"\"\"\n    color = imageio.v2.imread(rgb_frame_path)\n\n    if normalize:\n        color = color / 255.\n\n    return color\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.convert_angle_axis_to_matrix3","title":"<code>convert_angle_axis_to_matrix3(angle_axis)</code>","text":"<p>Converts a rotation from angle-axis representation to a 3x3 rotation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>angle_axis</code> <code>ndarray</code> <p>A 3-element array representing the rotation in angle-axis form.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 3x3 rotation matrix representing the same rotation as the input angle-axis.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is not a valid 3-element numpy array.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def convert_angle_axis_to_matrix3(angle_axis):\n    \"\"\"\n    Converts a rotation from angle-axis representation to a 3x3 rotation matrix.\n\n    Args:\n        angle_axis (numpy.ndarray): A 3-element array representing the rotation in angle-axis form.\n\n    Returns:\n        (numpy.ndarray): A 3x3 rotation matrix representing the same rotation as the input angle-axis.\n\n    Raises:\n        ValueError: If the input is not a valid 3-element numpy array.\n    \"\"\"\n    # Check if input is a numpy array\n    if not isinstance(angle_axis, np.ndarray):\n        raise ValueError(\"Input must be a numpy array.\")\n\n    # Check if the input is of shape (3,)\n    if angle_axis.shape != (3,):\n        raise ValueError(\"Input must be a 3-element array representing the rotation in angle-axis representation.\")\n\n    matrix, jacobian = cv2.Rodrigues(angle_axis)\n    return matrix\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.convert_matrix3_to_angle_axis","title":"<code>convert_matrix3_to_angle_axis(matrix)</code>","text":"<p>Converts a 3x3 rotation matrix to angle-axis representation (rotation vector).</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>A 3x3 rotation matrix representing the rotation.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 3-element array representing the rotation in angle-axis form</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is not a valid 3x3 numpy array.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def convert_matrix3_to_angle_axis(matrix):\n    \"\"\"\n    Converts a 3x3 rotation matrix to angle-axis representation (rotation vector).\n\n    Args:\n        matrix (numpy.ndarray): A 3x3 rotation matrix representing the rotation.\n\n    Returns:\n        (numpy.ndarray): A 3-element array representing the rotation in angle-axis form\n\n    Raises:\n        ValueError: If the input is not a valid 3x3 numpy array.\n    \"\"\"\n    # Check if input is a numpy array\n    if not isinstance(matrix, np.ndarray):\n        raise ValueError(\"Input must be a numpy array.\")\n\n    # Check if the input is of shape (3, 3)\n    if matrix.shape != (3, 3):\n        raise ValueError(\"Input must be a 3x3 matrix representing the rotation.\")\n\n    # Convert the 3x3 rotation matrix to an angle-axis (rotation vector)\n    angle_axis, jacobian = cv2.Rodrigues(matrix)\n\n    return angle_axis.flatten()  # Return as a 1D array (rotation vector)\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.decide_pose","title":"<code>decide_pose(pose)</code>","text":"<p>Determines the orientation of a 3D pose based on the alignment of its z-vector with predefined orientations.</p> <p>Parameters:</p> Name Type Description Default <code>pose</code> <code>ndarray</code> <p>A 4x4 NumPy array representing a 3D pose transformation matrix.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index representing the closest predefined orientation:  0 for upright, 1 for left, 2 for upside-down, and 3 for right.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def decide_pose(pose):\n    \"\"\"\n    Determines the orientation of a 3D pose based on the alignment of its z-vector with predefined orientations.\n\n    Args:\n        pose (np.ndarray): A 4x4 NumPy array representing a 3D pose transformation matrix.\n\n    Returns:\n        (int): Index representing the closest predefined orientation:\n             0 for upright, 1 for left, 2 for upside-down, and 3 for right.\n    \"\"\"\n\n    # pose style\n    z_vec = pose[2, :3]\n    z_orien = np.array(\n        [\n            [0.0, -1.0, 0.0], # upright\n            [-1.0, 0.0, 0.0], # left\n            [0.0, 1.0, 0.0], # upside-down\n            [1.0, 0.0, 0.0], # right\n        ]  \n    )\n    corr = np.matmul(z_orien, z_vec)\n    corr_max = np.argmax(corr)\n    return corr_max\n</code></pre>"},{"location":"toolkit/data-parser/#scenefun3d.utils.data_parser.rotate_pose","title":"<code>rotate_pose(im, rot_index)</code>","text":"<p>Rotates an image by a specified angle based on the rotation index.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>The input image to be rotated. It should have shape (height, width, channels).</p> required <code>rot_index</code> <code>int</code> <p>Index representing the rotation angle:              0 for no rotation, 1 for 90 degrees clockwise rotation,              2 for 180 degrees rotation, and 3 for 90 degrees counterclockwise rotation.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The rotated image.</p> Source code in <code>scenefun3d/utils/data_parser.py</code> <pre><code>def rotate_pose(im, rot_index):\n    \"\"\"\n    Rotates an image by a specified angle based on the rotation index.\n\n    Args:\n        im (numpy.ndarray): The input image to be rotated. It should have shape (height, width, channels).\n        rot_index (int): Index representing the rotation angle:\n                         0 for no rotation, 1 for 90 degrees clockwise rotation,\n                         2 for 180 degrees rotation, and 3 for 90 degrees counterclockwise rotation.\n\n    Returns:\n        (numpy.ndarray): The rotated image.\n    \"\"\"\n    h, w, d = im.shape\n    if d == 3:\n        if rot_index == 0:\n            new_im = im\n        elif rot_index == 1:\n            new_im = cv2.rotate(im, cv2.ROTATE_90_CLOCKWISE)\n        elif rot_index == 2:\n            new_im = cv2.rotate(im, cv2.ROTATE_180)\n        elif rot_index == 3:\n            new_im = cv2.rotate(im, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    return new_im\n</code></pre>"},{"location":"toolkit/example-scripts/","title":"Example scripts","text":"<p>As part of the toolkit, we provide basic example scripts to demonstrate how to utilize the data assets and tools. In this section, we outline the steps to run these scripts.</p>"},{"location":"toolkit/example-scripts/#example-1-projecting-image-rgb-onto-the-laser-scan","title":"Example 1: Projecting image RGB onto the laser scan","text":"<p>You can run as</p> <pre><code>python -m examples.project_rgb_on_laser_scan --data_dir &lt;data_dir&gt; --visit_id &lt;visit_id&gt; --video_id &lt;video_id&gt; --coloring_asset &lt;coloring_asset&gt; --crop_extraneous\n</code></pre> <p>where the supported arguments are:</p> <ul> <li> <p><code>--data_dir &lt;data_dir&gt;</code>: Specify the path of the data</p> </li> <li> <p><code>--visit_id &lt;visit_id&gt;</code>: Specify the identifier of the scene</p> </li> <li> <p><code>--video_id &lt;video_id&gt;</code>: Specify the identifier of the video sequence</p> </li> <li> <p><code>--coloring_asset &lt;coloring_asset&gt;</code>: Specify the RGB data asset to use for projecting the color to the laser scan. Can be <code>hires_wide</code> or <code>lowres_wide</code>. Defaults to <code>hires_wide</code>.</p> </li> <li> <p><code>--crop_extraneous</code>: Specify whether to crop the extraneous points from the laser scan based on the <code>crop_mask</code> asset.</p> </li> </ul>"},{"location":"toolkit/installation/","title":"Install dependencies","text":"<p>Download the code repository: <pre><code>git clone https://github.com/SceneFun3D/scenefun3d.git\ncd scenefun3d\n</code></pre></p> <p>Create virtual environment: <pre><code>conda create --name scenefun3d python=3.10\nconda activate scenefun3d\npip install -r requirements.txt\n</code></pre></p>"},{"location":"toolkit/overview/","title":"SceneFun3D ToolKit","text":"<p>The SceneFun3D ToolKit is available in this Github repository. It provides tools to</p> <ul> <li>Download the data</li> <li>Parse the data</li> <li>Visualization helpers</li> <li>Evaluation code for the benchmarks</li> <li>Example codes</li> </ul>"},{"location":"toolkit/visualization-helpers/","title":"Visualization","text":"<p>Here, we provide the documentation for the visualization helpers.</p>"},{"location":"toolkit/visualization-helpers/#scenefun3d.utils.viz.viz_3d","title":"<code>viz_3d(to_plot_list, viz_tool='pyviz3d')</code>","text":"<p>Visualize 3D point clouds.</p> <p>This function takes a list of 3D geometries (e.g., point clouds) and visualizes them using Open3D's visualization tools. An optional coordinate system can be displayed for reference.</p> <p>Parameters:</p> Name Type Description Default <code>to_plot_list</code> <code>list of open3d.geometry.PointCloud</code> <p>A list of Open3D point cloud geometry objects to be visualized.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This function does not return any value. It opens a visualizer window displaying the geometries.</p> Example <p>import open3d as o3d</p> <p>pcd = o3d.geometry.PointCloud()</p> <p>pcd.points = o3d.utility.Vector3dVector(np.random.rand(100, 3))</p> <p>viz_3d([pcd])</p> Source code in <code>scenefun3d/utils/viz.py</code> <pre><code>def viz_3d(to_plot_list, viz_tool=\"pyviz3d\"):\n    \"\"\"\n    Visualize 3D point clouds.\n\n    This function takes a list of 3D geometries (e.g., point clouds) and visualizes them using Open3D's visualization tools.\n    An optional coordinate system can be displayed for reference.\n\n    Args:\n        to_plot_list (list of open3d.geometry.PointCloud): A list of Open3D point cloud geometry objects to be visualized.\n\n    Returns:\n        (None): This function does not return any value. It opens a visualizer window displaying the geometries.\n\n    Example:\n        &gt;&gt;&gt; import open3d as o3d\n\n        &gt;&gt;&gt; pcd = o3d.geometry.PointCloud()\n\n        &gt;&gt;&gt; pcd.points = o3d.utility.Vector3dVector(np.random.rand(100, 3))\n\n        &gt;&gt;&gt; viz_3d([pcd])\n    \"\"\"\n    if viz_tool not in VIZ_TOOL_OPTIONS:\n        assert False, f\"Unknown viz tool option {viz_tool}. Visualization tool option must only be 'open3d' or 'pyviz3d'.\"\n\n    if viz_tool == \"pyviz3d\":\n        v = viz.Visualizer()\n        for i, plot_list_item in enumerate(to_plot_list):\n            pcd = copy.deepcopy(plot_list_item)\n\n            if not pcd.has_normals():\n                pcd = pc_estimate_normals(pcd, radius = 0.1, max_nn = 50)\n\n            pcd_points = np.array(pcd.points) \n            pcd_points -= np.mean(pcd_points, axis=0)\n            pcd.points = o3d.utility.Vector3dVector(pcd_points)\n\n            v.add_points(f'element_{i}', np.array(pcd.points), np.array(pcd.colors)*255., np.array(pcd.normals), point_size=8, visible=True)\n        v.save('pyviz3d_output')\n    else:\n        o3d.visualization.draw_geometries(to_plot_list)\n</code></pre>"},{"location":"toolkit/visualization-helpers/#scenefun3d.utils.viz.viz_masks","title":"<code>viz_masks(laser_scan_pcd, mask_indices, mask_labels=None, use_normals=True, viz_tool='pyviz3d')</code>","text":"<p>Visualize point cloud masks.</p> <p>Parameters:</p> Name Type Description Default <code>laser_scan_pcd</code> <code>PointCloud</code> <p>The original point cloud to be visualized.</p> required <code>mask_indices</code> <code>list of lists of int or list of np.array of int</code> <p>A list where each element is a list (or a np.array) of indices representing a mask in the point cloud.</p> required <code>mask_labels</code> <code>list of str</code> <p>Labels for each mask to be used with Pyviz3D. Must be the same length as mask_indices. Default is None.</p> <code>None</code> <code>use_normals</code> <code>bool</code> <p>Whether to use normals in the visualization. If True and the point cloud does not have normals, they will be estimated. Default is True.</p> <code>True</code> <code>viz_tool</code> <code>str</code> <p>The visualization tool to use, either 'pyviz3d' or 'open3d'. Default is 'pyviz3d'.</p> <code>'pyviz3d'</code> <p>Returns:</p> Type Description <code>None</code> <p>The function visualizes the point cloud masks and does not return any value.</p> Source code in <code>scenefun3d/utils/viz.py</code> <pre><code>def viz_masks(laser_scan_pcd, mask_indices, mask_labels = None, use_normals=True, viz_tool=\"pyviz3d\"):\n    \"\"\"\n    Visualize point cloud masks.\n\n    Args:\n        laser_scan_pcd (open3d.geometry.PointCloud): The original point cloud to be visualized.\n        mask_indices (list of lists of int or list of np.array of int): A list where each element is a list (or a np.array) of indices representing a mask in the point cloud.\n        mask_labels (list of str, optional): Labels for each mask to be used with Pyviz3D. Must be the same length as mask_indices. Default is None.\n        use_normals (bool, optional): Whether to use normals in the visualization. If True and the point cloud does not have normals, they will be estimated. Default is True.\n        viz_tool (str, optional): The visualization tool to use, either 'pyviz3d' or 'open3d'. Default is 'pyviz3d'.\n\n    Returns:\n        (None): The function visualizes the point cloud masks and does not return any value.\n    \"\"\"\n    if viz_tool not in VIZ_TOOL_OPTIONS:\n        assert False, f\"Unknown viz tool option {viz_tool}. Visualization tool option must only be 'open3d' or 'pyviz3d'.\"\n\n    if mask_labels is not None and viz_tool == \"open3d\":\n        assert False, f\"The mask_labels input is only supported for the visualization option 'pyviz3d'\"\n\n    if mask_labels is not None:\n        assert len(mask_indices) == len(mask_labels), f\"The length of mask_labels must be equal to the length of mask_indices. Each label must correspond to a single mask.\"\n\n    pcd = copy.deepcopy(laser_scan_pcd)\n    if not pcd.has_normals() and use_normals:\n        pcd = pc_estimate_normals(pcd, radius = 0.1, max_nn = 50)\n\n    # center the point cloud\n    pcd_points = np.array(pcd.points) \n    pcd_points -= np.mean(pcd_points, axis=0)\n    pcd.points = o3d.utility.Vector3dVector(pcd_points)\n\n    if viz_tool == \"pyviz3d\":\n        assert use_normals, f\"use_normals must be set to True if the visualization tool option is 'pyviz3d'.\"\n        v = viz.Visualizer()\n        v.add_points('RGB Color', np.array(pcd.points), np.array(pcd.colors)*255., np.array(pcd.normals), point_size=8, visible=False)\n\n    pcd_colors = np.array(pcd.colors) \n    new_pcd_colors = np.ones((pcd_colors.shape))*0+ (0.77, 0.77, 0.77)\n\n    if viz_tool == \"pyviz3d\":\n        v.add_points('Background Color', np.array(pcd.points), new_pcd_colors*255., np.array(pcd.normals), point_size=8, visible=True)\n\n    COLOR_MAP = list(SCANNET_COLOR_MAP_200.values())\n\n    for annot_i, mask_idx in enumerate(mask_indices):\n        cur_color = COLOR_MAP[annot_i % len(COLOR_MAP)]\n\n        new_pcd_colors[mask_idx] = cur_color \n        new_pcd_colors[mask_idx] /= 255.\n\n        if viz_tool == \"pyviz3d\":\n            mask_points = np.array(pcd.points)[mask_idx]\n            mask_colors = new_pcd_colors[mask_idx]\n            mask_normals = np.array(pcd.normals)[mask_idx]\n\n            if mask_labels is not None:\n                cur_label = f\"{mask_labels[annot_i]}_{annot_i}\"\n            else:\n                cur_label = f\"label_{annot_i}\"\n\n            if \"exclude\" in cur_label:\n                v.add_points(cur_label, mask_points, mask_colors*255., mask_normals, point_size=8, visible=False)\n            else:\n                v.add_points(cur_label, mask_points, mask_colors*255., mask_normals, point_size=8, visible=True)\n\n    if viz_tool == \"pyviz3d\":\n        v.save('pyviz3d_output')\n    else:\n        pcd.colors = o3d.utility.Vector3dVector(new_pcd_colors)\n        viz_3d([pcd])\n</code></pre>"},{"location":"toolkit/visualization-helpers/#scenefun3d.utils.viz.viz_motions","title":"<code>viz_motions(laser_scan_pcd, motion_types, motion_dirs, motion_origins, motion_viz_orients, motion_labels=None)</code>","text":"<p>Visualize point cloud masks.</p> <p>Parameters:</p> Name Type Description Default <code>laser_scan_pcd</code> <code>PointCloud</code> <p>The original point cloud to be visualized.</p> required <code>motion_types</code> <code>list of str</code> <p>A list where each element represents the motion type. Motion type can be either \"trans\" for translational motions or \"rot\" for rotational motions.</p> required <code>motion_dirs</code> <code>list of np.array</code> <p>A list where each element represents the motion direction.</p> required <code>motion_origins</code> <code>list of np.array</code> <p>A list where each element represents the motion origin.</p> required <code>motion_viz_orients</code> <code>list of str</code> <p>A list where each element represents the motion orientation. Motion orientation can be either \"inwards\" or \"outwards\".</p> required <code>motion_labels</code> <code>list of str</code> <p>Labels for each mask to be used with Pyviz3D.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>The function visualizes the point cloud masks and does not return any value.</p> Source code in <code>scenefun3d/utils/viz.py</code> <pre><code>def viz_motions(laser_scan_pcd, motion_types, motion_dirs, motion_origins, motion_viz_orients, motion_labels = None):\n    \"\"\"\n    Visualize point cloud masks.\n\n    Args:\n        laser_scan_pcd (open3d.geometry.PointCloud): The original point cloud to be visualized.\n        motion_types (list of str): A list where each element represents the motion type. Motion type can be either \"trans\" for translational motions or \"rot\" for rotational motions.\n        motion_dirs (list of np.array): A list where each element represents the motion direction.\n        motion_origins (list of np.array): A list where each element represents the motion origin.\n        motion_viz_orients (list of str): A list where each element represents the motion orientation. Motion orientation can be either \"inwards\" or \"outwards\".\n        motion_labels (list of str, optional): Labels for each mask to be used with Pyviz3D.\n\n    Returns:\n        (None): The function visualizes the point cloud masks and does not return any value.\n    \"\"\"\n    pcd = copy.deepcopy(laser_scan_pcd)\n    if not pcd.has_normals():\n        pcd = pc_estimate_normals(pcd, radius = 0.1, max_nn = 50)\n\n    # center the point cloud\n    pcd_points = np.array(pcd.points) \n    pcd_mean = np.mean(pcd_points, axis=0)\n    pcd.points = o3d.utility.Vector3dVector(pcd_points - pcd_mean)\n\n    v = viz.Visualizer()\n    v.add_points('RGB Color', np.array(pcd.points), np.array(pcd.colors)*255., np.array(pcd.normals), point_size=8, visible=False)\n\n    pcd_colors = np.array(pcd.colors) \n    new_pcd_colors = np.ones((pcd_colors.shape))*0+ (0.77, 0.77, 0.77)\n\n    if motion_labels is not None:\n        motion_labels = [f\"{motion_labels[i]}_{i}\" for i in range(len(motion_labels))]\n    else:\n        motion_labels = [f\"motion_{i}\" for i in range(len(motion_types))]\n\n    v.add_points('Background Color', np.array(pcd.points), new_pcd_colors*255., np.array(pcd.normals), point_size=8, alpha=.8, visible=True)\n\n    COLOR_MAP = list(SCANNET_COLOR_MAP_200.values())\n    for idx, (m_type, m_dir, m_origin, m_orient, m_label) in \\\n        enumerate(zip(motion_types, motion_dirs, motion_origins, motion_viz_orients, motion_labels)):\n\n        cur_color = np.array(COLOR_MAP[idx % len(COLOR_MAP)]) / 255.\n        v.add_motion(\n            m_label, \n            m_type, \n            np.array(m_dir), \n            np.array(m_origin - pcd_mean), \n            m_orient, \n            cur_color, \n            cur_color, \n            visible=True\n        )\n        breakpoint()\n\n    v.save('pyviz3d_output')\n</code></pre>"}]}